{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rebound model\n",
    "\n",
    "Aim: Quantify the environmental impact due to the savings of households in consumption expenses, across different \n",
    "- industrial sectors and scenarios:\n",
    "    - housing (rent): baseline for 2011, \n",
    "    - energy: efficient_devices, renewable_energy \n",
    "    - food-waste: avoidable_waste_saving\n",
    "    - clothing: sufficiency, refuse, reshare, reuse for 2025  \n",
    "    - furnishing: refuse, reuse for 2035 and 2050 \n",
    "- temporal periods: years 2006-2017 \n",
    "- spatial regions: parts of Switzerland\n",
    "\n",
    "\n",
    "_Input_: The household budet survey files to train the data \n",
    "\n",
    "_Model_: A random forest or Artificial neural network model \n",
    "\n",
    "_Output_: The rebound expenses and environmental footprints of the households \n",
    "\n",
    "TOC<a id=\"toc\"></a>\n",
    "\n",
    "- <a href=\"#ini\"> Step 0: Initialisation</a>\n",
    "- <a href=\"#preprocess\"> Step 1: Preprocessing</a>\n",
    "- <a href=\"#model\"> Step 2: Model </a>\n",
    "- <a href=\"#post\"> Step 3: Postprocessing </a>\n",
    "- <a href=\"#lca\"> Step 4: LCA </a>\n",
    "\n",
    "\n",
    "Author: Rhythima Shinde, ETH Zurich\n",
    "\n",
    "Co-Authors (for energy case study and temporal-regional rebound studies): Sidi Peng, Saloni Vijay, ETH Zurich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 0. Initialisation <a id = 'ini'></a>\n",
    "\n",
    "<a href=\"#toc\">back</a>\n",
    "\n",
    "### 0.1. Input files & data parameters\n",
    "- (1a) **seasonal_file** -> For the year 2009-11, the file is provided by <a href= https://pubs.acs.org/doi/full/10.1021/acs.est.8b01452>A.Froemelt</a>. It is modified based on original HBS(HABE) data that we <a href = https://www.bfs.admin.ch/bfs/en/home/statistics/economic-social-situation-population/surveys/hbs.html>obtain from Federal Statistical Office of Switzerland</a>. It is further modiefied in this code in the <a href='#preprocess'>preprocessing section</a> to rename columns.\n",
    "- (1b) **seasonal_file_SI** -> Lists the HBS data columns and associated activities to calculate the consumption based environmental footprint. <a href=https://pubs.acs.org/doi/abs/10.1021/acs.est.8b01452>The file can be found here.</a>\n",
    "- (2) **habe_month** -> the HBS household ids and their derivation to the month and year of the survey filled \n",
    "- (3) dependent_indices -> based on the HBS column indices, this file lists the relevant consumption expense parameters which are predicted \n",
    "- (4) **independent_indices** -> the HBS column indices which define the household socio-economic properties\n",
    "- (5) **target_data** -> Selects the target dataset to predict the results. For most cases, it is the subset of the HBS (for the housing industry, it is the partner dataset 'ABZ', 'SCHL' or 'SM')  \n",
    "- (6) **directory_name** -> based on the industry case, changes the dependent parameters, and income saved by the household (due to which the rebound is supposed to happen) - change the second value in the list. \n",
    "\n",
    "### 0.2. Model parameters\n",
    "- (1) **iter_n** -> no.of iterations of runs\n",
    "- (2) **model_name** -> Random Forest (RF) or ANN (Artificial Neural Network)\n",
    "\n",
    "### 0.3. Analysis parameters\n",
    "- (1) industry change: directory_name with following dependencies \n",
    "    - scenarios, \n",
    "    - partner_name/target dataset,\n",
    "    - idx_column_savings_cons,\n",
    "    - dependent_indices\n",
    "- (2) year change: seasonal_file\n",
    "    - specify which years (2006, 2007, 2008... 2017)\n",
    "- (3) regional change: target_dataset\n",
    "    - specify which regions (DE, IT, FR, ZH)\n",
    "    - specify partner name (ABZ, SCHL, SM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style='color:blue'>USER INPUT NEEDED: chose model settings, methods of preprocessing </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and folder settings\n",
    "directory_name = 'housing' # 'housing' or 'furniture' or 'clothing' or 'energy'        \n",
    "iter_n=1\n",
    "model_name='RF' # 'RF' or 'ANN'\n",
    "\n",
    "## preprocessing methods\n",
    "option_deseason = 'deseasonal' # 'deseasonal' [option 1] or 'month-ind' [option 2]\n",
    "if option_deseason ==  'month-ind':\n",
    "    n_ind = 63\n",
    "    independent_indices='raw_data/independent_month.csv' \n",
    "if option_deseason == 'deseasonal':\n",
    "    n_ind = 39\n",
    "    independent_indices='raw_data/independent.csv' \n",
    "input_normalise = 'no-normalise' #'no-normalise' for not normalising the data or 'normalise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.multioutput as sko\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "from sklearn.metrics import r2_score,mean_squared_error, explained_variance_score\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import brightway2\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd, MultiComparison\n",
    "import statsmodels.api as sm\n",
    "from functools import reduce\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# Additional libraries for neural network implementation \n",
    "\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(2)\n",
    "# from keras import optimizers\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Read the modified files by Nauser et al (2020)\n",
    "# - HBS data (merged raw HBS files) \"HABE_mergerd_2006_2017\" \n",
    "# - tranlsation file 'HABE_Cname_translator.xlsx'\n",
    "# - HBS hhids with the corresponding month of the survey \n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "seasonal_file = 'raw_data/HBS/HABE_merged_2006_2017.csv'\n",
    "seasonal_file_SI = 'raw_data/HBS/HABE_Cname_translator.xlsx'\n",
    "habe_month = 'raw_data/HBS/HABE_date.csv'\n",
    "inf_index_file = 'raw_data/HBS/HABE_inflation_index_all.xlsx'\n",
    "# seasonal_file = 'original_Andi_HBS/habe20092011_hh_prepared_imputed.csv' #based on the years\n",
    "# seasonal_file_SI='original_Andi_HBS/Draft_Paper_8_v11_SupportingInformation.xlsx' \n",
    "# habe_month='original_Andi_HBS/habe_hh_month.csv'\n",
    "\n",
    "\n",
    "## form the databases\n",
    "df_habe = pd.read_csv(seasonal_file, delimiter=',', error_bad_lines=False, encoding='ISO-8859–1')\n",
    "df_habe_month = pd.read_csv(habe_month, delimiter=',', error_bad_lines=False, encoding='ISO-8859–1')\n",
    "inf_index = pd.read_excel(inf_index_file)\n",
    "\n",
    "dependent_indices= 'raw_data/dependent_'+directory_name+'.csv'\n",
    "dependent_indices_pd = pd.read_csv(dependent_indices, delimiter=',', encoding='ISO-8859–1')\n",
    "dependent_indices_pd_name = pd.read_csv(dependent_indices,sep=',')[\"name\"]\n",
    "dependentsize=len(list(dependent_indices_pd_name))\n",
    "\n",
    "independent_indices_pd = pd.read_csv(independent_indices, delimiter=',', encoding='ISO-8859–1')\n",
    "list_independent_columns = pd.read_csv(independent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()\n",
    "list_dependent_columns = pd.read_csv(dependent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add more columns to perform temporal analysis (month_names and time_periods)\n",
    "\n",
    "def label_month (row):\n",
    "    if row['month'] == 1.0 :\n",
    "        return 'January'\n",
    "    if row['month'] == 2.0 :\n",
    "        return 'February'\n",
    "    if row['month'] == 3.0 :\n",
    "        return 'March'\n",
    "    if row['month'] == 4.0 :\n",
    "        return 'April'\n",
    "    if row['month'] == 5.0 :\n",
    "        return 'May'\n",
    "    if row['month'] == 6.0 :\n",
    "        return 'June'\n",
    "    if row['month'] == 7.0 :\n",
    "        return 'July'\n",
    "    if row['month'] == 8.0 :\n",
    "        return 'August'\n",
    "    if row['month'] == 9.0 :\n",
    "        return 'September'\n",
    "    if row['month'] == 10.0 :\n",
    "        return 'October'\n",
    "    if row['month'] == 11.0 :\n",
    "        return 'November'\n",
    "    if row['month'] == 12.0 :\n",
    "        return 'December'\n",
    "    \n",
    "def label_period (row):\n",
    "    if (row[\"year\"] == 2006) or (row[\"year\"] == 2007) or (row[\"year\"] == 2008):\n",
    "        return '1'\n",
    "    if (row[\"year\"] == 2009) or (row[\"year\"] == 2010) or (row[\"year\"] == 2011):\n",
    "        return '2'\n",
    "    if (row[\"year\"] == 2012) or (row[\"year\"] == 2013) or (row[\"year\"] == 2014):\n",
    "        return '3'\n",
    "    if (row[\"year\"] == 2015) or (row[\"year\"] == 2016) or (row[\"year\"] == 2017):\n",
    "        return '4'\n",
    "    \n",
    "df_habe_month['month_name']=df_habe_month.apply(lambda row: label_month(row), axis=1)\n",
    "\n",
    "df_habe_month['period']=df_habe_month.apply(lambda row: label_month(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'color:red'> TODO: update the right values for energy and food industry scenarios, and then merge in the above script </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory_name =='housing':\n",
    "    scenarios = {'baseline_2011':500}\n",
    "    target_data ='ABZ'\n",
    "#     target_data = 'subset-HBS'\n",
    "    idx_column_savings_cons = 'net_rent_and_mortgage_interest_of_principal_residence' #289  \n",
    "    \n",
    "if directory_name == 'furniture':\n",
    "    scenarios = {'refuse_2035':17,'refuse_2050':17.4,'reuse_1_2035':6.9,\n",
    "                           'reuse_1_2050':8.2,'reuse_2_2035':10.2,'reuse_2_2050':9.5}\n",
    "    target_data = 'subset-HBS' \n",
    "    idx_column_savings_cons = 'furniture_and_furnishings,_carpets_and_other_floor_coverings_incl._repairs' #313  \n",
    "    \n",
    "if directory_name == 'clothing':\n",
    "    scenarios = {'sufficiency_2025':76.08,'refuse_2025':5.7075,'share_2025':14.2875,'local_reuse_best_2025':9.13,\n",
    "                         'local_reuse_worst_2025':4.54,'max_local_reuse_best_2025':10.25,'max_local_reuse_worst_2025':6.83}\n",
    "    target_data = 'subset-HBS' \n",
    "    idx_column_savings_cons = 'clothing' #248     \n",
    "    \n",
    "if directory_name == 'energy':  \n",
    "    scenarios = {'efficient_devices':30,'renewable_energy':300}\n",
    "    target_data = 'subset-HBS' \n",
    "    idx_column_savings_cons = 'energy_of_principal_residence' #297 \n",
    "    \n",
    "if directory_name == 'food':  \n",
    "    scenarios = {'avoidable_waste_saving':50}\n",
    "    target_data = 'subset-HBS' \n",
    "    idx_column_savings_cons = 'food_and_non_alcoholic_beverages' #97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to make relevant sector-wise directories\n",
    "\n",
    "def make_pre_directory(outname,directory_name):\n",
    "    outdir = 'preprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname\n",
    "\n",
    "def make_pre_sub_directory(outname,directory_name,sub_dir):\n",
    "    outdir = 'preprocessing/'+directory_name+'/'+sub_dir\n",
    "    outdir1 = 'preprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir1):\n",
    "        os.mkdir(outdir1)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname\n",
    "\n",
    "def make_pre_sub_sub_directory(outname,directory_name,sub_dir,sub_sub_dir):\n",
    "    outdir='preprocessing/'+directory_name+'/'+sub_dir+'/'+sub_sub_dir\n",
    "    outdir1 = 'preprocessing/'+directory_name+'/'+sub_dir\n",
    "    outdir2 = 'preprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir2):\n",
    "        os.mkdir(outdir2)\n",
    "    if not os.path.exists(outdir1):\n",
    "        os.mkdir(outdir1)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing <a id = 'preprocess' ></a>\n",
    "\n",
    "TOC: <a id = 'toc-pre-pre'></a>\n",
    "- <a href = #rename>1.1. Prepare training data</a>\n",
    "- <a href = #deseasonal>1.2. Deseasonalise</a>\n",
    "- <a href = #normal>1.3. Normalize</a>\n",
    "- <a href = #check>1.4. Checks</a>\n",
    "\n",
    "### 1.1. Prepare training data <a id='rename'></a>\n",
    "\n",
    "<a href='#toc-pre'>back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Rename HBS columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_translate = pd.read_excel(seasonal_file_SI, sheet_name='translator', header=3, \n",
    "                              usecols=['habe_code', 'habe_eng_p', 'habe_eng', 'vcode', 'qcode'])\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.strip()\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.replace(' ', '_')\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.replace('-', '_')\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.replace('\"', '')\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.lower()\n",
    "var_translate['habe_code'] = var_translate['habe_code'].str.lower()\n",
    "dict_translate = dict(zip(var_translate['habe_code'], var_translate['habe_eng']))\n",
    "df_habe.rename(columns=dict_translate, inplace=True)\n",
    "dict_translate = dict(zip(var_translate['qcode'], var_translate['habe_eng']))\n",
    "df_habe.rename(columns=dict_translate, inplace=True)\n",
    "df_habe_rename = df_habe.loc[:, ~df_habe.columns.duplicated()]\n",
    "pd.DataFrame.to_csv(df_habe_rename, 'preprocessing/0_habe_rename.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Inflation adjustment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_habe_rename = pd.read_csv('preprocessing/0_habe_rename.csv')\n",
    "df_new = pd.merge(df_habe_rename, df_habe_month, on='haushaltid')\n",
    "pd.DataFrame.to_csv(df_new,'preprocessing/0_habe_rename_month.csv', sep=',',index=False)\n",
    "\n",
    "list_var_total = dependent_indices_pd_name.tolist()\n",
    "list_var_total.pop()\n",
    "\n",
    "# monetary variables inflation adjusted\n",
    "list_mon = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "list_year = [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
    "list_var_total = list_var_total + [\"disposable_income\", \"total_expenditures\"]\n",
    "# , 'infrequent_income'] \n",
    "\n",
    "df_inf = df_new\n",
    "\n",
    "for col in list_var_total:\n",
    "    for year in list_year:\n",
    "        for mon in list_mon:\n",
    "            df_inf.loc[(df_inf['year'] == year) & (df_inf['month'] == mon), col] = \\\n",
    "                df_inf.loc[(df_inf['year'] == year) & (df_inf['month'] == mon), col] / \\\n",
    "                inf_index.loc[(inf_index['year'] == year) & (inf_index['month'] == mon), col].values * 100\n",
    "\n",
    "pd.DataFrame.to_csv(df_inf, 'preprocessing/1_habe_inflation.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Adapt the columns (optional - one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_columns(xx,directory_name):\n",
    "    pd_df_saved = df_inf\n",
    "    pd_df_saved.loc[:,'disposable_income'] = pd_df_saved['disposable_income'] - pd_df_saved.loc[:,xx] \n",
    "#     pd_df_saved['total_expenditures'] = pd_df_saved['total_expenditures'] - pd_df_saved.iloc[:,313]\n",
    "    fullname = make_pre_directory('1_habe_rename_new_columns.csv',directory_name)\n",
    "    pd.DataFrame.to_csv(pd_df_saved,fullname, sep=',',index=False)\n",
    "    return pd_df_saved\n",
    "\n",
    "df_habe_rename_saved = new_columns(idx_column_savings_cons,directory_name) # when redefining disposable income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4. Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers():\n",
    "    df_outliers = df_habe_rename_saved  # TODO if using the new definition of disposable income: use the df_habe_rename_saved\n",
    "#     df_outliers = df_outliers[np.abs(stats.zscore(df_outliers['disposable_income']))<10] \n",
    "#     df_outliers = df_outliers[np.abs(stats.zscore(df_outliers['saved_amount_(computed)']))<10]\n",
    "    df_outliers = df_outliers[df_outliers['disposable_income'] >= 0] # simply keep all the 'sensible' disposable incomes\n",
    "    # df_outliers = df_outliers[df_outliers['disposable_income'] <= 14800]  # ADDED CRITERIA FOR REMOVING OUTLIERS OF THE DISP_INCOME\n",
    "    # df_outliers = df_outliers[df_outliers['total_expenditures'] >= 0]  # simply keep all the 'sensible' total_expenses\n",
    "    df_outliers = df_outliers[df_outliers['saved_amount_(computed)'] >= 0]\n",
    "    fullname = make_pre_directory('2_habe_rename_removeoutliers.csv',directory_name)\n",
    "    pd.DataFrame.to_csv(df_outliers, fullname, sep=',', index=False)\n",
    "    return df_outliers\n",
    "\n",
    "df_habe_outliers = remove_outliers()\n",
    "\n",
    "## aggregate the data as per the categories \n",
    "def accumulate_categories_habe(df,new_column,file_name):\n",
    "    list_dependent_columns = pd.read_csv(dependent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()\n",
    "    list_dependent_columns_new = list_dependent_columns\n",
    "    list_dependent_columns_new.append('disposable_income')\n",
    "    list_dependent_columns_new.append(new_column) # Might not always need this\n",
    "    \n",
    "    df = df[list_dependent_columns_new]\n",
    "    df = df.loc[:,~df.columns.duplicated()] #drop duplicates\n",
    "    \n",
    "    df[new_column] = df.iloc[:, [17]]\n",
    "    df['income'] = df.iloc[:, [16]]\n",
    "    df['food'] = df.iloc[:,[0,1,2]].sum(axis=1)\n",
    "    df['misc'] = df.iloc[:,[3,4]].sum(axis=1)\n",
    "    df['housing'] = df.iloc[:, [5, 6]].sum(axis=1)\n",
    "    df['services'] = df.iloc[:, [7,8,9]].sum(axis=1)\n",
    "    df['travel'] = df.iloc[:, [10,11,12, 13, 14]].sum(axis=1)\n",
    "    df['savings'] = df.iloc[:, [15]]    \n",
    "    df = df[['income','food','misc','housing','services','travel','savings',new_column]]\n",
    "    \n",
    "    fullname = make_pre_directory(file_name,directory_name)\n",
    "    pd.DataFrame.to_csv(df,fullname,sep=',',index= False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_outliers = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv')\n",
    "df_habe_accumulate = accumulate_categories_habe(df_outliers,'month_name','2_habe_rename_removeoutliers_aggregated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Deasonalising <a id='deseasonal'></a>\n",
    "- [Option 1] Clustering based on months \n",
    "- [Option 2] Use month and period as independent variable\n",
    "\n",
    "<a href = #toc-pre-pre>back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. [Option 1] Create monthly datasets,  Plots/ Tables / Statistical tests for HABE monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if option_deseason == 'deseasonal' :\n",
    "    \n",
    "    def split_month():\n",
    "        df_new = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv')\n",
    "        df_month = df_new.groupby('month_name')\n",
    "\n",
    "        for i in range(12):\n",
    "            df_new_month=pd.DataFrame(list(df_month)[i][1])\n",
    "            df_new_month['month_name']=df_new_month['month_name'].astype('str')\n",
    "            fullname=make_pre_sub_directory('3_habe_monthly_'+df_new_month.month_name.unique()[0]+'.csv',\n",
    "                                            directory_name,option_deseason)\n",
    "            pd.DataFrame.to_csv(df_new_month,fullname,sep=',', index = False)\n",
    "\n",
    "    split_month()\n",
    "\n",
    "    # Split the accumulated categories per month\n",
    "\n",
    "    def split_month_accumulated():\n",
    "        df_new = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers_aggregated.csv',sep=',')\n",
    "        df_month = df_new.groupby('month_name')\n",
    "        for i in range(12):\n",
    "            df_new_month=pd.DataFrame(list(df_month)[i][1])\n",
    "            df_new_month['month_name']=df_new_month['month_name'].astype('str')\n",
    "            fullname = make_pre_sub_directory('3_habe_monthly_'+df_new_month.month_name.unique()[0]+'_aggregated.csv',\n",
    "                                              directory_name,option_deseason)\n",
    "            pd.DataFrame.to_csv(df_new_month,fullname, sep=',', index = False)\n",
    "\n",
    "    split_month_accumulated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. [Option 1] Making final clusters <a id ='finalclusters'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'color:blue'>USER INPUT NEEDED: edit the cluster-list below</p>\n",
    "\n",
    "<p style = 'color:red'>TODO - join clusters based on the p-values calculated above directly</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## current clusters are made based on the mean table above\n",
    "\n",
    "if option_deseason == 'deseasonal' :\n",
    "\n",
    "    Cluster_month_lists  = {1:('January',),2:('February','March','April'),3:('May','June','July'),\n",
    "                           4:('August','September','October','November'),5:('December',)}\n",
    "    cluster_number_length = len(Cluster_month_lists)\n",
    "\n",
    "    for key in Cluster_month_lists:\n",
    "        df1=[]\n",
    "        df_sum=[]\n",
    "        for i in range(0,len(Cluster_month_lists[key])):\n",
    "            print(Cluster_month_lists[key])\n",
    "            df=pd.read_csv(make_pre_sub_directory('3_habe_monthly_{}'.format(Cluster_month_lists[key][i])+'.csv',\n",
    "                                                  directory_name,option_deseason))\n",
    "            df_sum.append(df.shape[0])\n",
    "            df1.append(df)\n",
    "        df_cluster = pd.concat(df1)\n",
    "        assert df_cluster.shape[0]==sum(df_sum) # to check if the conacting was done correctly\n",
    "        pd.DataFrame.to_csv(df_cluster,make_pre_sub_directory('4_habe_monthly_cluster_'+str(key)+'.csv',\n",
    "                                                              directory_name,option_deseason),sep=',')\n",
    "\n",
    "# TODO: update this to move to the sub directory of deseaspnal files\n",
    "#     cluster_number_length = len(Cluster_month_lists)\n",
    "#     for i in list(range(1,cluster_number_length+1)):\n",
    "#         accumulate_categories_habe(df,'number_of_persons_per_household','4_habe_monthly_cluster_'+str(i)+'_aggregated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Option 2: Month as independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if option_deseason == 'month-ind' :\n",
    "    cluster_number_length = 1\n",
    "    \n",
    "    # do one-hot encoding for month and year\n",
    "    hbs_all = pd.read_csv('preprocessing/'+directory_name+'/1_habe_rename_new_columns.csv')\n",
    "    month_encoding = pd.get_dummies(hbs_all.month_name, prefix='month')\n",
    "    year_encoding = pd.get_dummies(hbs_all.year, prefix='year')\n",
    "    hbs_all_encoding = pd.concat([hbs_all, month_encoding.reindex(month_encoding.index)], axis=1)\n",
    "    hbs_all_encoding = pd.concat([hbs_all_encoding, year_encoding.reindex(year_encoding.index)], axis=1)\n",
    "    \n",
    "    for key in scenarios:\n",
    "        output_encoding = make_pre_sub_sub_directory('3_habe_for_all_scenarios_encoding.csv',\n",
    "                                                     directory_name,option_deseason,key)\n",
    "        pd.DataFrame.to_csv(hbs_all_encoding,output_encoding,sep=',',index=False)\n",
    "    \n",
    "    month_name = month_encoding.columns.tolist()\n",
    "    year_name = year_encoding.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Normalisation  <a id='normal'></a>\n",
    "\n",
    "<a href='#toc-pre'>back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Normalisation of HBS and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NORMALISATION \n",
    "\n",
    "# if input_normalise == 'normalise':\n",
    "#     def normalise_habe(cluster):\n",
    "#         transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "        \n",
    "#         if option_deseason == 'deseasonal':\n",
    "#             df_deseasonal_file = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "#                                              '/4_habe_monthly_cluster_'+str(cluster)+'.csv', \n",
    "#                                              delimiter=',')\n",
    "#         if option_deseason == 'month-ind':\n",
    "#             df_deseasonal_file = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "#                                              '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "            \n",
    "#         pd_df_new = df_deseasonal_file\n",
    "\n",
    "#         for colsss in list_dependent_columns:\n",
    "#             pd_df_new[[colsss]] = transformer.transform(df_deseasonal_file[[colsss]])\n",
    "\n",
    "#         for colsss in list_independent_columns:\n",
    "#             min_colsss = df_deseasonal_file[[colsss]].quantile([0.01]).values[0]\n",
    "#             max_colsss = df_deseasonal_file[[colsss]].quantile([0.99]).values[0]\n",
    "#             pd_df_new[[colsss]] = (df_deseasonal_file[[colsss]] - min_colsss) / (max_colsss - min_colsss)\n",
    "\n",
    "#         pd_df = pd_df_new[list_independent_columns+['haushaltid']+list_dependent_columns]\n",
    "#         pd_df = pd_df.fillna(0)\n",
    "#         fullname = make_pre_directory('4_habe_deseasonal_'+str(cluster)+'_'+str(option_deseason)+'_normalised.csv',\n",
    "#                                       directory_name)\n",
    "#         pd.DataFrame.to_csv(pd_df,fullname,sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if target_data == 'ABZ':\n",
    "#     if input_normalise =='normalise':\n",
    "#         def normalise_partner(i,key,option_deseason):\n",
    "#             pd_df_partner = pd.read_csv('target_'+target_data+'.csv',delimiter=',')\n",
    "#             df_complete =  pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv',delimiter=',') \n",
    "#             pd_df_partner['disposable_income'] = pd_df_partner['disposable_income'] + i\n",
    "\n",
    "#             for colsss in list_independent_columns:\n",
    "#                 min_colsss = df_complete[[colsss]].quantile([0.01]).values[0]\n",
    "#                 max_colsss = df_complete[[colsss]].quantile([0.99]).values[0]\n",
    "#                 pd_df_partner[[colsss]] = (pd_df_partner[[colsss]] - min_colsss) / (max_colsss - min_colsss)\n",
    "\n",
    "#             # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,30]<=1]\n",
    "#             # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,32]<=1]\n",
    "#             # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,33]>=0] #todo remove rows with normalisation over the range\n",
    "\n",
    "#             fullname = make_pre_sub_sub_directory('5_final_'+ target_data + '_independent_final_'+str(i)+'.csv',\n",
    "#                                               directory_name,option_deseason,key)\n",
    "\n",
    "#             pd.DataFrame.to_csv(pd_df_partner,fullname,sep=',',index=False)\n",
    "#             return pd_df_partner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Preprocessing without normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_normalise == 'no-normalise': \n",
    "    def normalise_habe(cluster):\n",
    "        transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "\n",
    "        if option_deseason == 'deseasonal':\n",
    "            df_deseasonal_file = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "                                             '/4_habe_monthly_cluster_'+str(cluster)+'.csv', \n",
    "                                             delimiter=',')\n",
    "        if option_deseason == 'month-ind':\n",
    "            df_deseasonal_file = pd.read_csv('preprocessing/'+directory_name+ '/' + str(option_deseason) + '/' + str(key) +\n",
    "                                             '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "           \n",
    "        pd_df_new = df_deseasonal_file\n",
    "\n",
    "        pd_df = pd_df_new[list_independent_columns+['haushaltid']+list_dependent_columns]\n",
    "        pd_df = pd_df.fillna(0)\n",
    "        fullname = make_pre_sub_directory('4_habe_deseasonal_'+str(cluster)+'_short.csv',\n",
    "                                      directory_name,option_deseason)\n",
    "        pd.DataFrame.to_csv(pd_df,fullname,sep=',',index=False)\n",
    "        \n",
    "for i in list(range(1,cluster_number_length+1)):\n",
    "    df_normalise_habe_file = normalise_habe(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collecting the independent and dependent datasets\n",
    "\n",
    "def truncate_all(key):\n",
    "    if option_deseason == 'deseasonal':\n",
    "        df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv', \n",
    "                                         delimiter=',', error_bad_lines=False)\n",
    "    if option_deseason == 'month-ind':\n",
    "        df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + str(option_deseason) + '/' + str(key) +\n",
    "                                             '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "    \n",
    "    df_habe_imputed_clustered_d = df_seasonal_normalised[list_dependent_columns]\n",
    "    df_habe_imputed_clustered_i = df_seasonal_normalised[list_independent_columns]\n",
    "    \n",
    "    fullname_d = make_pre_sub_sub_directory('raw_dependent.csv',directory_name,option_deseason,key)\n",
    "    fullname_in = make_pre_sub_sub_directory('raw_independent.csv',directory_name,option_deseason,key)\n",
    "    \n",
    "    pd.DataFrame.to_csv(df_habe_imputed_clustered_d,fullname_d,sep=',',index=False)\n",
    "    pd.DataFrame.to_csv(df_habe_imputed_clustered_i,fullname_in,sep=',',index=False)\n",
    "\n",
    "for key in scenarios:\n",
    "    truncate_all(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NORMALISATION \n",
    "\n",
    "if target_data == 'subset-HBS':\n",
    "    def normalise_partner(i,key,option_deseason):\n",
    "        N = 300 # TODO pass this as an argument when chosing subset of HBS\n",
    "        pd_df_partner = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/raw_independent.csv', \n",
    "                                    delimiter=',', error_bad_lines=False)\n",
    "        pd_df_partner = pd_df_partner.sample(frac=0.4, replace=True, random_state=1)\n",
    "        pd_df_partner['disposable_income'] = pd_df_partner['disposable_income']+i\n",
    "        fullname = make_pre_sub_sub_directory('5_final_'+ target_data + '_independent_final_'+str(i)+'.csv',\n",
    "                                          directory_name,option_deseason,key)\n",
    "\n",
    "        pd.DataFrame.to_csv(pd_df_partner,fullname,sep=',',index=False)\n",
    "        return pd_df_partner\n",
    "    \n",
    "\n",
    "if target_data == 'ABZ':\n",
    "    if input_normalise =='no-normalise':\n",
    "        def normalise_partner(i,key,option_deseason):\n",
    "            pd_df_partner = pd.read_csv('raw_data/target_'+target_data+'.csv',delimiter=',')\n",
    "            df_complete =  pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv',delimiter=',') \n",
    "            pd_df_partner['disposable_income'] = pd_df_partner['disposable_income'] - i\n",
    "\n",
    "            # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,30]<=1]\n",
    "            # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,32]<=1]\n",
    "            # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,33]>=0] #todo remove rows with normalisation over the range\n",
    "\n",
    "            fullname = make_pre_sub_sub_directory('5_final_'+ target_data + '_independent_final_'+str(i)+'.csv',\n",
    "                                              directory_name,option_deseason,key)\n",
    "\n",
    "            pd.DataFrame.to_csv(pd_df_partner,fullname,sep=',',index=False)\n",
    "            return pd_df_partner\n",
    "\n",
    "for key in scenarios:\n",
    "    list_incomechange=[0,scenarios[key]]\n",
    "    for i in list_incomechange:\n",
    "        df_normalise_partner_file = normalise_partner(i,key,option_deseason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Checks<a id='check'></a>\n",
    "\n",
    "<a href='#toc-pre'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_normalise =='normalise':\n",
    "    def truncate(cluster_number):\n",
    "        if option_deseason == 'deseasonal':\n",
    "            df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "                                                 '/4_habe_deseasonal_'+str(cluster_number)+'_normalised.csv', \n",
    "                                                 delimiter=',', error_bad_lines=False)\n",
    "        if option_deseason == 'month-ind':\n",
    "            df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + str(option_deseason) + '/' + str(key) +\n",
    "                                             '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "        df_habe_imputed_clustered_d = df_seasonal_normalised[list_dependent_columns]\n",
    "        df_habe_imputed_clustered_dl = np.expm1(df_habe_imputed_clustered_d)\n",
    "        df_habe_imputed_clustered_i = df_seasonal_normalised[list_independent_columns]\n",
    "        \n",
    "        fullname_dl = make_pre_sub_sub_directory('raw_dependent_old_'+str(cluster_number)+'.csv',directory_name,\n",
    "                                                 'checks',option_deseason)\n",
    "        fullname_d = make_pre_sub_sub_directory('raw_dependent_'+str(cluster_number)+'.csv',directory_name,\n",
    "                                                'checks',option_deseason)\n",
    "        fullname_in = make_pre_sub_sub_directory('raw_independent_'+str(cluster_number)+'.csv',directory_name,\n",
    "                                                 'checks',option_deseason)\n",
    "        pd.DataFrame.to_csv(df_habe_imputed_clustered_dl,fullname_dl,sep=',',index=False)\n",
    "        pd.DataFrame.to_csv(df_habe_imputed_clustered_d,fullname_d,sep=',',index=False)\n",
    "        pd.DataFrame.to_csv(df_habe_imputed_clustered_i,fullname_in,sep=',',index=False)\n",
    "    \n",
    "if input_normalise =='no-normalise':\n",
    "    def truncate(cluster_number):\n",
    "        if option_deseason == 'deseasonal':\n",
    "            df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "                                                 '/4_habe_deseasonal_'+str(cluster_number)+'_short.csv', \n",
    "                                                 delimiter=',', error_bad_lines=False)\n",
    "        if option_deseason == 'month-ind':\n",
    "            df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + str(option_deseason) + '/' + str(key) +\n",
    "                                             '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "        df_habe_imputed_clustered_d = df_seasonal_normalised[list_dependent_columns]\n",
    "        df_habe_imputed_clustered_i = df_seasonal_normalised[list_independent_columns]\n",
    "        \n",
    "        fullname_d = make_pre_sub_sub_directory('raw_dependent_'+str(cluster_number)+'.csv',directory_name,\n",
    "                                            'checks',option_deseason)\n",
    "        fullname_in = make_pre_sub_sub_directory('raw_independent_'+str(cluster_number)+'.csv',directory_name,\n",
    "                                             'checks',option_deseason)\n",
    "        pd.DataFrame.to_csv(df_habe_imputed_clustered_d,fullname_d,sep=',',index=False)\n",
    "        pd.DataFrame.to_csv(df_habe_imputed_clustered_i,fullname_in,sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(1,cluster_number_length+1)):\n",
    "    truncate(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MODEL <a id = \"model\"></a>\n",
    "    \n",
    "<a href = \"#toc\">back</a>\n",
    "\n",
    "TOC:<a id ='toc-model'></a>\n",
    "- <a href = \"#prep\"> 2.1. Prepare train-test-target datasets</a>\n",
    "- <a href = \"#predict\"> 2.2. Prediction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Prepare train-test-target datasets <a id ='prep'></a>\n",
    "\n",
    "<a href=#toc-model>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_haushalts(values,id_ix=0):\n",
    "    haushalts = dict()\n",
    "    haushalt_ids = np.unique(values[:,id_ix])\n",
    "    for haushalt_id in haushalt_ids:\n",
    "        selection = values[:, id_ix] == haushalt_id\n",
    "        haushalts[haushalt_id] = values[selection]\n",
    "    return haushalts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(haushalts,length_training,month_name,row_in_chunk):\n",
    "    train, test = list(), list()\n",
    "    cut_point = int(0.8*length_training)  # 0.9*9754 # declare cut_point as per the size of the imputed database #TODO check if this is too less\n",
    "    print('Month/cluster and cut_point',month_name, cut_point)\n",
    "    for k,rows in haushalts.items():\n",
    "        train_rows = rows[rows[:,row_in_chunk] < cut_point, :]\n",
    "        test_rows = rows[rows[:,row_in_chunk] > cut_point, :]\n",
    "        train.append(train_rows[:, :])\n",
    "        test.append(test_rows[:, :])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NORMALISATION\n",
    "\n",
    "if input_normalise =='normalise':\n",
    "\n",
    "    def df_habe_train_test(df,month_name,length_training):\n",
    "        df=df.assign(id_split = list(range(df.shape[0])))\n",
    "        train, test = split_train_test(to_haushalts(df.values),length_training,month_name,row_in_chunk=df.shape[1]-1)\n",
    "\n",
    "        train_rows = np.array([row for rows in train for row in rows])\n",
    "        test_rows = np.array([row for rows in test for row in rows])\n",
    "\n",
    "        independent = list(range(0,independent_indices_pd.shape[0]))\n",
    "        dependent =  list(range(independent_indices_pd.shape[0]+1,\n",
    "                                independent_indices_pd.shape[0]+dependent_indices_pd.shape[0]+1))\n",
    "\n",
    "        trained_independent = train_rows[:, independent]\n",
    "        trained_dependent = train_rows[:, dependent]\n",
    "        test_independent = test_rows[:, independent]\n",
    "        test_dependent = test_rows[:, dependent]\n",
    "\n",
    "        ## OPTIONAL lines FOR CHECK - comment if not needed\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/trained_dependent_nonexp.csv', \n",
    "                   trained_dependent, delimiter=',')    \n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/trained_dependent.csv', \n",
    "                   np.expm1(trained_dependent),delimiter=',')\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/trained_independent.csv', \n",
    "                   trained_independent, delimiter=',')\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/test_dependent.csv', \n",
    "                   np.expm1(test_dependent), delimiter=',')\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/test_independent.csv', \n",
    "                   test_independent, delimiter=',')\n",
    "\n",
    "        return trained_independent,trained_dependent,test_independent,test_dependent\n",
    "\n",
    "    def df_partner_test(y):\n",
    "        df_partner = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/5_final_' + target_data + \n",
    "                                 '_independent_final_' + str(y) + '.csv',delimiter=',')\n",
    "        length_training = df_partner.shape[0]\n",
    "        train_partner, test_partner = split_train_test(to_haushalts(df_partner.values),length_training,month_name,1) \n",
    "        train_rows_partner = np.array([row for rows in train_partner for row in rows])\n",
    "        new_independent = list(range(0, n_ind)) # number of columns of the independent parameters\n",
    "        train_partner_independent = train_rows_partner[:, new_independent]\n",
    "\n",
    "        ### Optional lines for CHECK - comment if not needed\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/train_partner_independent_' + model_name + '_' + str(y) + '.csv',\n",
    "                   train_partner_independent, delimiter=',')\n",
    "\n",
    "        return train_partner_independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## form the train test datasets \n",
    "\n",
    "# NO-NORMALISATION\n",
    "if input_normalise =='no-normalise':\n",
    "    def df_habe_train_test(df,month_name,length_training):\n",
    "        df=df.assign(id_split = list(range(df.shape[0])))\n",
    "        train, test = split_train_test(to_haushalts(df.values),length_training,month_name,row_in_chunk=df.shape[1]-1)\n",
    "\n",
    "        train_rows = np.array([row for rows in train for row in rows])\n",
    "        test_rows = np.array([row for rows in test for row in rows])\n",
    "\n",
    "        independent = list(range(0,independent_indices_pd.shape[0]))\n",
    "        dependent =  list(range(independent_indices_pd.shape[0]+1,\n",
    "                                independent_indices_pd.shape[0]+dependent_indices_pd.shape[0]+1))\n",
    "\n",
    "        trained_independent = train_rows[:, independent]\n",
    "        trained_dependent = train_rows[:, dependent]\n",
    "        test_independent = test_rows[:, independent]\n",
    "        test_dependent = test_rows[:, dependent]\n",
    "\n",
    "        ## OPTIONAL lines FOR CHECK - comment if not needed\n",
    "        # np.savetxt('raw/checks/trained_dependent_nonexp_'+str(month_name)+'.csv', trained_dependent, delimiter=',')    \n",
    "        # np.savetxt('raw/checks/trained_independent_nonexp_'+str(month_name)+'.csv', trained_independent, delimiter=',')\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/test_dependent_'+str(month_name)+'.csv', \n",
    "                   test_dependent,delimiter=',')    \n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/test_independent_'+str(month_name)+'.csv', \n",
    "                   test_independent, delimiter=',')\n",
    "\n",
    "        return trained_independent,trained_dependent,test_independent,test_dependent\n",
    "    \n",
    "    def df_partner_test(y):\n",
    "        df_partner = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/5_final_' + target_data + \n",
    "                                 '_independent_final_' + str(y) + '.csv', delimiter=',')\n",
    "        length_training = df_partner.shape[0]\n",
    "        train_partner, test_partner = split_train_test(to_haushalts(df_partner.values),\n",
    "                                                       length_training,cluster_number,1) \n",
    "        train_rows_partner = np.array([row for rows in train_partner for row in rows])\n",
    "        new_independent = list(range(0, n_ind))\n",
    "        train_partner_independent = train_rows_partner[:, new_independent]\n",
    "\n",
    "        ### Optional lines for CHECK - comment if not needed\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/train_partner_independent_' + \n",
    "                   model_name + '_' + str(y) + '.csv', train_partner_independent, delimiter=',')\n",
    "\n",
    "        return train_partner_independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_post_directory(outname,directory_name):\n",
    "    outdir = 'postprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname\n",
    "\n",
    "def make_post_sub_directory(outname,directory_name,sub_dir):\n",
    "    outdir_1='postprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir_1):\n",
    "        os.mkdir(outdir_1)\n",
    "    outdir = 'postprocessing/'+directory_name+'/'+sub_dir\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname\n",
    "\n",
    "def make_post_sub_sub_directory(outname,directory_name,sub_dir,sub_sub_dir):\n",
    "    outdir_1='postprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir_1):\n",
    "        os.mkdir(outdir_1)\n",
    "    outdir = 'postprocessing/'+directory_name+'/'+sub_dir\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    outdir_2='postprocessing/'+directory_name+'/'+sub_dir+'/'+sub_sub_dir\n",
    "    if not os.path.exists(outdir_2):\n",
    "        os.mkdir(outdir_2)\n",
    "    fullname = os.path.join(outdir_2, outname)\n",
    "    return fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR NO NORMALISATION AND TEST DATA \n",
    "\n",
    "def df_test(y,cluster_number):\n",
    "    pd_df_partner = pd.read_csv('raw/checks/trained_independent_'+str(cluster_number)+'.csv', delimiter=',', header = None)\n",
    "    pd_df_partner.iloc[:,-1] = pd_df_partner.iloc[:,-1] + y\n",
    "\n",
    "    pd.DataFrame.to_csv(pd_df_partner, 'raw/checks/5_trained_independent_'+str(cluster_number)+'_'+str(y)+'.csv', \n",
    "                        sep=',',index=False)\n",
    "    return pd_df_partner\n",
    "\n",
    "def df_stratified_test(y):\n",
    "    pd_df_partner = pd.read_csv('raw/checks/5_setstratified_independent_1_'+str(y)+'.csv', delimiter=',')\n",
    "    return pd_df_partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using Neural Networks\n",
    "\n",
    "# def ANN():\n",
    "#     nn = Sequential()\n",
    "#     nn.add(Dense(39,kernel_initializer='normal',activation=\"relu\",input_shape=(39,)))\n",
    "#     nn.add(Dense(50,kernel_initializer='normal',activation=\"relu\"))\n",
    "#     nn.add(Dense(100,kernel_initializer='normal',activation=\"relu\"))\n",
    "#     nn.add(Dense(100,kernel_initializer='normal',activation=\"relu\") )\n",
    "#     # nn.add(Dense(100,kernel_initializer='normal',activation=\"relu\"))\n",
    "#     # nn.add(Dense(100,kernel_initializer='normal',activation=\"relu\"))\n",
    "#     nn.add(Dense(dependentsize,kernel_initializer='normal')) #,kernel_constraint=min_max_norm(min_value=0.01,max_value=0.05)))\n",
    "#     sgd = optimizers.SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#     nn.compile(optimizer=sgd, loss='mean_squared_error', metrics=['accuracy'])\n",
    "#     return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Clustered Prediction <a id='predict'></a>\n",
    "\n",
    "<a href='#toc-model'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NORMALISATION\n",
    "\n",
    "if input_normalise =='normalise':\n",
    "\n",
    "    def fit_predict_cluster(i,y,cluster_number,key):\n",
    "        df = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+\n",
    "                         '/4_habe_deseasonal_'+str(cluster_number)+'_normalised.csv',\n",
    "                         delimiter=',',error_bad_lines=False, encoding='ISO-8859–1')\n",
    "        length_training = df.shape[0]\n",
    "        trained_independent, trained_dependent, test_independent, test_dependent = df_habe_train_test(df,\n",
    "                                                                                                      str(cluster_number),\n",
    "                                                                                                      length_training)\n",
    "        train_partner_independent = df_partner_test(y)\n",
    "        \n",
    "        if model_name == 'ANN':\n",
    "            estimator = KerasRegressor(build_fn=ANN)\n",
    "            estimator.fit(trained_independent, trained_dependent, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "            ### PREDICTION FROM HERE\n",
    "            prediction_nn = estimator.predict(train_partner_independent)\n",
    "            prediction_nn_denormalised = np.expm1(prediction_nn)\n",
    "            fullname = make_post_sub_sub_directory('predicted_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) + '.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn_denormalised, delimiter=',')\n",
    "\n",
    "            ### TEST PREDICTION\n",
    "            prediction_nn_test = estimator.predict(test_independent)\n",
    "            prediction_nn_test_denormalised = np.expm1(prediction_nn_test)\n",
    "            fullname = make_post_sub_sub_directory('predicted_test' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) + '.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn_test_denormalised, delimiter=',')\n",
    "\n",
    "            ### CROSS VALIDATION FROM HERE\n",
    "            kfold = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "            results1 = cross_val_score(estimator, test_independent, test_dependent, cv=kfold)\n",
    "            print(\"Results_test: %.2f (%.2f)\" % (results1.mean(), results1.std()))\n",
    "\n",
    "        if model_name == 'RF':\n",
    "            estimator = sko.MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_features=n_ind, random_state=30))\n",
    "            estimator.fit(trained_independent, trained_dependent)\n",
    "\n",
    "            ### PREDICTION FROM HERE\n",
    "            prediction_nn = estimator.predict(train_partner_independent)\n",
    "            results0 = estimator.oob_score\n",
    "            prediction_nn_denormalised = np.expm1(prediction_nn)\n",
    "            fullname = make_post_sub_sub_directory('predicted_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) + '.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn_denormalised, delimiter=',')\n",
    "\n",
    "             ### TEST PREDICTION\n",
    "            prediction_nn_test = estimator.predict(test_independent)\n",
    "            prediction_nn_test_denormalised = np.expm1(prediction_nn_test)\n",
    "            fullname = make_post_sub_sub_directory('predicted_test' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) + '.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn_test_denormalised, delimiter=',')        \n",
    "\n",
    "            #### CROSS VALIDATION FROM HERE\n",
    "            kfold = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "            # results0 = estimator.oob_score\n",
    "            # results1 = cross_val_score(estimator, test_independent, test_dependent, cv=kfold)\n",
    "            results2 = r2_score(test_dependent,prediction_nn_test)\n",
    "            results3 = mean_squared_error(test_dependent,prediction_nn_test)\n",
    "            results4 = explained_variance_score(test_dependent,prediction_nn_test)\n",
    "            # print(\"cross_val_score: %.2f (%.2f)\" % (results1.mean(), results1.std()))\n",
    "            # print(\"oob_r2_score: %.2f \" % results0)\n",
    "            print(\"r2_score: %.2f \" % results2)\n",
    "            print(\"mean_squared_error: %.2f \" % results3)\n",
    "            print(\"explained_variance_score: %.2f \" % results4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR NO NORMALISATION\n",
    "\n",
    "if input_normalise =='no-normalise':\n",
    "    def fit_predict_cluster(i,y,cluster_number,key):\n",
    "        df_non_normalised = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/4_habe_deseasonal_'+\n",
    "                                        str(cluster_number)+ '_short.csv', delimiter=',',\n",
    "                                        error_bad_lines=False, encoding='ISO-8859–1')\n",
    "        length_training = df_non_normalised.shape[0]\n",
    "        print(length_training)\n",
    "        trained_independent, trained_dependent, test_independent, test_dependent = df_habe_train_test(df_non_normalised,\n",
    "                                                                                                      str(cluster_number),\n",
    "                                                                                                      length_training)\n",
    "        train_partner_independent = df_partner_test(y)\n",
    "        \n",
    "        ### Additional for the HBS test data subset\n",
    "        # test_new_independent = df_test(y,1) # chosing just one cluster here\n",
    "        # sratified_independent = df_stratified_test(y)\n",
    "    \n",
    "        if model_name == 'ANN':\n",
    "            estimator = KerasRegressor(build_fn=ANN)\n",
    "            estimator.fit(trained_independent, trained_dependent, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "            ### PREDICTION FROM HERE\n",
    "            prediction_nn = estimator.predict(train_partner_independent)\n",
    "            fullname = make_post_sub_sub_directory('predicted_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) +'.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn, delimiter=',')\n",
    "\n",
    "            ### TEST PREDICTION\n",
    "            prediction_nn_test = estimator.predict(test_independent)\n",
    "            fullname = make_post_sub_sub_directory('predicted_test_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) +'.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn_test, delimiter=',')\n",
    "\n",
    "            ### CROSS VALIDATION FROM HERE\n",
    "            kfold = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "            results1 = cross_val_score(estimator, test_independent, test_dependent, cv=kfold)\n",
    "            print(\"Results_test: %.2f (%.2f)\" % (results1.mean(), results1.std()))\n",
    "\n",
    "        if model_name == 'RF':\n",
    "            estimator = sko.MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_features=n_ind, random_state=30))\n",
    "            estimator.fit(trained_independent, trained_dependent)\n",
    "            \n",
    "            ### FEATURE IMPORTANCE\n",
    "            rf = RandomForestRegressor()\n",
    "            rf.fit(trained_independent, trained_dependent)\n",
    "            FI = rf.feature_importances_\n",
    "            list_independent_columns = pd.read_csv(independent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()\n",
    "            independent_columns = pd.DataFrame(list_independent_columns)\n",
    "            FI_names = pd.DataFrame(FI)\n",
    "            FI_names = pd.concat([independent_columns, FI_names], axis=1)\n",
    "            FI_names.columns = ['independent_variables', 'FI_score']\n",
    "            pd.DataFrame.to_csv(FI_names,'preprocessing/'+directory_name+'/8_habe_feature_importance'+ '_' +\n",
    "                                str(y) + '_' + str(i) + '_' + str(cluster_number) +'.csv', sep=',',index= False)\n",
    "            FI_names_sorted = FI_names.sort_values('FI_score', ascending = False)\n",
    "#             print(FI_names_sorted)\n",
    "\n",
    "            ### PREDICTION FROM HERE\n",
    "            prediction_nn = estimator.predict(train_partner_independent)\n",
    "            fullname = make_post_sub_sub_directory('predicted_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) +'.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn, delimiter=',')\n",
    "\n",
    "             ### TEST PREDICTION\n",
    "            prediction_nn_test = estimator.predict(test_independent)\n",
    "            fullname = make_post_sub_sub_directory('predicted_test_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) +'.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn_test, delimiter=',')     \n",
    "\n",
    "            #### CROSS VALIDATION FROM HERE\n",
    "            kfold = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "            \n",
    "            for i in range(16):\n",
    "                column_predict = pd.DataFrame(test_dependent).iloc[:,i]\n",
    "                model = sm.OLS(column_predict, test_independent).fit() \n",
    "                print(i)\n",
    "                print('standard error=',model.bse) \n",
    "            \n",
    "            # results0 = estimator.oob_score\n",
    "            # results1 = cross_val_score(estimator, test_independent, test_dependent, cv=kfold)\n",
    "#             results2 = r2_score(test_dependent,prediction_nn_test)\n",
    "#             results3 = mean_squared_error(test_dependent,prediction_nn_test)\n",
    "#             results4 = explained_variance_score(test_dependent,prediction_nn_test)\n",
    "            # print(\"cross_val_score: %.2f (%.2f)\" % (results1.mean(), results1.std()))\n",
    "            # print(\"oob_r2_score: %.2f \" % results0)\n",
    "            print(\"r2_score: %.2f \" % results2)\n",
    "            print(\"mean_squared_error: %.2f \" % results3)\n",
    "            print(\"explained_variance_score: %.2f \" % results4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER of MONTHS - PREDICTIONS\n",
    "for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "    print(cluster_number)\n",
    "    for j in range(0, iter_n):\n",
    "        for key in scenarios:\n",
    "            list_incomechange=[0,scenarios[key]]\n",
    "            for y in list_incomechange:\n",
    "                fit_predict_cluster(j,y,cluster_number,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.POSTPROCESSING <a id = \"post\"></a>\n",
    "    \n",
    "<a href=\"#toc\">back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Average of the clustered predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if option_deseason == 'month-ind': \n",
    "    df_habe_outliers = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/4_habe_deseasonal_'+\n",
    "                                            str(cluster_number)+ '_short.csv', delimiter=',')\n",
    "if option_deseason == 'deseasonal':\n",
    "    df_habe_outliers = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pandas_cluster(y,cluster_number,key):\n",
    "    df_all = []\n",
    "    df_trained_partner = pd.read_csv('preprocessing/'+directory_name+'/checks/'+option_deseason+'/train_partner_independent_'+\n",
    "                                     model_name+'_'+str(y)+'.csv')\n",
    "    for i in range(0,iter_n):\n",
    "        df = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + \n",
    "                         str(y) + '_' + str(i) + '_' + \n",
    "                         str(cluster_number) + '.csv', delimiter = ',', header=None)\n",
    "        df_all.append(df)\n",
    "    glued = pd.concat(df_all, axis=1, keys=list(map(chr,range(97,97+iter_n))))\n",
    "    glued = glued.swaplevel(0, 1, axis=1)\n",
    "    glued = glued.groupby(level=0, axis=1).mean()\n",
    "    glued_new = glued.reindex(columns=df_all[0].columns)\n",
    "\n",
    "    max_income = df_habe_outliers[['disposable_income']].quantile([0.99]).values[0]\n",
    "    min_income = df_habe_outliers[['disposable_income']].quantile([0.01]).values[0]\n",
    "    glued_new['income'] = df_trained_partner[df_trained_partner.columns[-1]]\n",
    "    pd.DataFrame.to_csv(glued_new, 'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y)\n",
    "                        + '_'+str(cluster_number)+'.csv', sep=',',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in scenarios:\n",
    "    list_incomechange=[0,scenarios[key]]\n",
    "    for y in list_incomechange:\n",
    "        for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "            average_pandas_cluster(y,cluster_number,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_categories_cluster(y,cluster_number):\n",
    "    df_income = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y)\n",
    "                            + '_'+str(cluster_number)+'.csv', \n",
    "                            sep=',',header=None)\n",
    "#     df_income['household_size'] = df_income.iloc[:, [17]]\n",
    "    df_income['income'] = df_income.iloc[:, [16]]\n",
    "    df_income['food'] = df_income.iloc[:,[0,1,2]].sum(axis=1)\n",
    "    df_income['misc'] = df_income.iloc[:,[3,4]].sum(axis=1)\n",
    "    df_income['housing'] = df_income.iloc[:, [5, 6]].sum(axis=1)\n",
    "    df_income['services'] = df_income.iloc[:, [7, 8, 9 ]].sum(axis=1)\n",
    "    df_income['travel'] = df_income.iloc[:, [10, 11, 12, 13, 14]].sum(axis=1)\n",
    "    df_income['savings'] = df_income.iloc[:, [15]]             \n",
    "    df_income = df_income[['income','food','misc','housing','services','travel','savings']]\n",
    "    pd.DataFrame.to_csv(df_income,\n",
    "                        'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y) \n",
    "                        + '_'+str(cluster_number)+'_aggregated.csv', sep=',',index=False)\n",
    "    return df_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in scenarios:\n",
    "    list_incomechange=[0,scenarios[key]]\n",
    "    for y in list_incomechange: \n",
    "        for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "            accumulate_categories_cluster(y,cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation of clusters\n",
    "\n",
    "list_dfs_month=[]\n",
    "for key in scenarios:\n",
    "    list_incomechange=[0,scenarios[key]]\n",
    "    for y in list_incomechange:    \n",
    "        for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "            pd_predicted_month = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y) \n",
    "                                             + '_'+str(cluster_number)+'_aggregated.csv', delimiter = ',')\n",
    "            list_dfs_month.append(pd_predicted_month)\n",
    "\n",
    "        df_concat = pd.concat(list_dfs_month,sort=False)\n",
    "\n",
    "        by_row_index = df_concat.groupby(df_concat.index)\n",
    "        df_means = by_row_index.mean()\n",
    "        pd.DataFrame.to_csv(df_means,'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y) + '_' + \n",
    "                            str(dependentsize) +'_aggregated.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Calculate differences/ rebounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dependent_columns = pd.read_csv(dependent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()\n",
    "\n",
    "def difference_new():\n",
    "    for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "        for key in scenarios:\n",
    "            list_incomechange=[0,scenarios[key]]\n",
    "            for i in range(0,iter_n):\n",
    "                df_trained_partner = pd.read_csv('preprocessing/'+directory_name+'/checks/'+'/'+option_deseason+'/train_partner_independent_'+\n",
    "                                     model_name+'_'+str(y)+'.csv',header=None)\n",
    "                df_500 = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_'\n",
    "                                     +str(list_incomechange[1])+ '_'+str(i)\n",
    "                                     + '_'+str(cluster_number)+'.csv', delimiter=',',header=None)\n",
    "                df_0 = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_0_' \n",
    "                                    + str(i)  + '_'+str(cluster_number)+ '.csv', delimiter=',',header=None)\n",
    "                df_500.columns = list_dependent_columns\n",
    "                df_0.columns = df_500.columns\n",
    "                df_diff = -df_500+df_0\n",
    "                if option_deseason == 'month-ind':\n",
    "                    df_diff['disposable_income']=df_trained_partner[df_trained_partner.columns[-25]]\n",
    "                if option_deseason == 'deseasonal':\n",
    "                    df_diff['disposable_income']=df_trained_partner[df_trained_partner.columns[-1]]\n",
    "                pd.DataFrame.to_csv(df_diff,'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name \n",
    "                                    + '_rebound_'+str(i)+ '_' + str(cluster_number) + '.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_clusters(key):\n",
    "    df_all = []\n",
    "    for i in range(0,iter_n):\n",
    "        df = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+ model_name + '_rebound_' + \n",
    "                         str(i)+ '_' + str(cluster_number)+'.csv',delimiter=',',index_col=None)\n",
    "        df_all.append(df)\n",
    "    \n",
    "    df_concat = pd.concat(df_all,sort=False)\n",
    "\n",
    "    by_row_index = df_concat.groupby(df_concat.index)\n",
    "    df_means = by_row_index.mean()\n",
    "    pd.DataFrame.to_csv(df_means, 'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name +'_rebound.csv',\n",
    "                        sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in scenarios:\n",
    "    average_clusters(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_categories(key):\n",
    "    df_income = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound.csv',delimiter=',')\n",
    "#     df_income['household_size'] = df_income.iloc[:, [17]]\n",
    "    df_income['income'] = df_income.iloc[:, [16]]\n",
    "    df_income['food'] = df_income.iloc[:,[0,1,2]].sum(axis=1)\n",
    "    df_income['misc'] = df_income.iloc[:,[3,4]].sum(axis=1)\n",
    "    df_income['housing'] = df_income.iloc[:, [5, 6]].sum(axis=1)\n",
    "    df_income['services'] = df_income.iloc[:, [7, 8, 9]].sum(axis=1)\n",
    "    df_income['travel'] = df_income.iloc[:, [10, 11, 12,13, 14]].sum(axis=1)\n",
    "    df_income['savings'] = df_income.iloc[:, [15]]    \n",
    "    df_income = df_income[['income','food','misc','housing','services','travel','savings']]#'transfers','total_sum'\n",
    "    data[key]=list(df_income.mean())\n",
    "    if list(scenarios.keys()).index(key) == len(scenarios)-1:\n",
    "        df = pd.DataFrame(data, columns = [key for key in scenarios],\n",
    "                  index=['income','food','misc','housing','services','travel','savings'])\n",
    "        print(df)\n",
    "        pd.DataFrame.to_csv(df.T, 'postprocessing/rebound_results_'+directory_name+ '_income.csv', sep=',',index=True)\n",
    "    pd.DataFrame.to_csv(df_income,\n",
    "                        'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound_aggregated.csv',\n",
    "                        sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "for key in scenarios:\n",
    "    accumulate_categories(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=('<2000','2000-4000','4000-6000','6000-8000','8000-10000','>10000')\n",
    "def income_group(row):\n",
    "    if row['disposable_income'] <= 2000:\n",
    "        return groups[0]\n",
    "    if row['disposable_income'] <= 4000:\n",
    "        return groups[1]\n",
    "    if row['disposable_income'] <= 6000:\n",
    "        return groups[2]\n",
    "    if row['disposable_income'] <= 8000:\n",
    "        return groups[3]\n",
    "    if row['disposable_income'] <= 10000:\n",
    "        return groups[4]\n",
    "    if row['disposable_income'] > 10000:\n",
    "        return groups[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_income_groups():\n",
    "    df_income = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound.csv',\n",
    "                            delimiter=',')\n",
    "    df_income['income_group'] = df_income.apply(lambda row: income_group(row), axis=1)\n",
    "    df_income_new = df_income.groupby(['income_group']).mean()\n",
    "    pd.DataFrame.to_csv(df_income_new,'postprocessing/rebound_results_'+directory_name+ '_income_categories.csv', sep=',',index=True)\n",
    "    pd.DataFrame.to_csv(df_income,\n",
    "                        'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound_income.csv',\n",
    "                        sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_income_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=('<2000','2000-4000','4000-6000','6000-8000','8000-10000','>10000')\n",
    "def income_group(row):\n",
    "    if row['income'] <= 2000 :\n",
    "        return groups[0]\n",
    "    if row['income'] <= 4000:\n",
    "        return groups[1]\n",
    "    if row['income'] <= 6000:\n",
    "        return groups[2]\n",
    "    if row['income'] <= 8000:\n",
    "        return groups[3]\n",
    "    if row['income'] <= 10000:\n",
    "        return groups[4]\n",
    "    if row['income'] > 10000:\n",
    "        return groups[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_income_groups_new():\n",
    "    df_income = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound_aggregated.csv',\n",
    "                            delimiter=',')\n",
    "    print(df_income.columns)\n",
    "    df_income['income_group'] = df_income.apply(lambda row: income_group(row), axis=1)\n",
    "    df_income_new = df_income.groupby(['income_group']).mean()\n",
    "    pd.DataFrame.to_csv(df_income_new,'postprocessing/rebound_results_'+directory_name+ '_categories.csv', sep=',',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_income_groups_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LCA <a id = \"lca\"></a>\n",
    "\n",
    "<a href = '#toc'>back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a file with associated impacts_per_FU for each HABE category: \n",
    "    - a. Get the ecoinvent data from brightway\n",
    "    - b. Get the exiobase data from direct file (Livia's)\n",
    "    - c. Attach the heia and Agribalyse values \n",
    "2. Convert the impact_per_FU to impact_per_expenses\n",
    "3. Run the following scripts to  \n",
    "    - (a) allocate the income category to each household in HBS (train data) and ABZ (target data) \n",
    "    - (b) calculate environmental impact per consumption main-category per income group as listed in the raw/dependent_10.csv\n",
    "        - (1) From HBS: % of expense of consumption sub-category per consumption main-category as listed in the raw/dependent_10.csv \n",
    "        - (2) expenses per FU of each consumption sub-category \n",
    "    - (c) From target data: Multiply the rebound results (consumption expenses) with the env. impact values above \n",
    "        based on the income of the household\n",
    "    \n",
    "    OR \n",
    "    \n",
    "    Use A.Kim's analysis here: https://github.com/aleksandra-kim/consumption_model for the calculation of impacts_per_FU for each HABE catergory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "file = open('LCA/contribution_scores_sectors_allfu1.pickle','rb')\n",
    "x = pickle.load(file)\n",
    "print(x)\n",
    "with open('LCA/impacts_per_FU_sectors.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x:\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "file = open('LCA/contribution_scores_5categories_allfu1.pickle','rb')\n",
    "x = pickle.load(file)\n",
    "print(x)\n",
    "with open('LCA/impacts_per_FU.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('LCA/contribution_scores_v2.pickle','rb')\n",
    "x1 = pickle.load(file)\n",
    "with open('LCA/impacts_per_FU.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x1.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('LCA/contribution_scores_sectors_allfu1.pickle','rb')\n",
    "x = pickle.load(file)\n",
    "with open('LCA/impacts_per_FU_sectors.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## TODO use the manually updated CHF/FU to calculate the income per expense \n",
    "df_expense = pd.read_csv('LCA/impacts_per_expense.csv',sep=',',index_col='sector')\n",
    "df_income_CHF = pd.read_csv('postprocessing/rebound_results_'+directory_name+ '_income.csv',sep=',')\n",
    "for i in ['food','travel','housing','food','misc','services']:\n",
    "    df_income_CHF[i+'_GHG']=df_expense.loc[i,'Average of GWP/CHF']*df_income_CHF[i]\n",
    "    pd.DataFrame.to_csv(df_income_CHF,'postprocessing/rebound_results_'+directory_name+ '_income_all_GHG.csv',sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rebound]",
   "language": "python",
   "name": "conda-env-rebound-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
