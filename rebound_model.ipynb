{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rebound model\n",
    "\n",
    "Aim: Quantify the environmental impact due to the savings of households in consumption expenses, across different \n",
    "- industrial sectors and scenarios:\n",
    "    - housing (rent): baseline for 2011, \n",
    "    - energy: efficient_devices, renewable_energy \n",
    "    - food-waste: avoidable_waste_saving\n",
    "    - clothing: sufficiency, refuse, reshare, reuse for 2025  \n",
    "    - furnishing: refuse, reuse for 2035 and 2050 \n",
    "- temporal periods: years 2006-2017 \n",
    "- spatial regions: parts of Switzerland\n",
    "\n",
    "\n",
    "_Input_: The household budet survey files to train the data \n",
    "\n",
    "_Model_: A random forest or Artificial neural network model \n",
    "\n",
    "_Output_: The rebound expenses and environmental footprints of the households \n",
    "\n",
    "TOC<a id=\"toc\"></a>\n",
    "\n",
    "- <a href=\"#ini\"> Step 0: Initialisation</a>\n",
    "- <a href=\"#preprocess\"> Step 1: Preprocessing</a>\n",
    "- <a href=\"#model\"> Step 2: Model </a>\n",
    "- <a href=\"#post\"> Step 3: Postprocessing </a>\n",
    "- <a href=\"#lca\"> Step 4: LCA </a>\n",
    "\n",
    "\n",
    "Author: Rhythima Shinde, ETH Zurich\n",
    "\n",
    "Co-Authors (for energy case study and temporal-regional rebound studies): Sidi Peng, Saloni Vijay, ETH Zurich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 0. Initialisation <a id = 'ini'></a>\n",
    "\n",
    "<a href=\"#toc\">back</a>\n",
    "\n",
    "### 0.1. Input files & data parameters\n",
    "- (1a) **seasonal_file** -> For the year 2009-11, the file is provided by <a href= https://pubs.acs.org/doi/full/10.1021/acs.est.8b01452>A.Froemelt</a>. It is modified based on original HBS(HABE) data that we <a href = https://www.bfs.admin.ch/bfs/en/home/statistics/economic-social-situation-population/surveys/hbs.html>obtain from Federal Statistical Office of Switzerland</a>. It is further modiefied in this code in the <a href='#preprocess'>preprocessing section</a> to rename columns.\n",
    "- (1b) **seasonal_file_SI** -> Lists the HBS data columns and associated activities to calculate the consumption based environmental footprint. <a href=https://pubs.acs.org/doi/abs/10.1021/acs.est.8b01452>The file can be found here.</a>\n",
    "- (2) **habe_month** -> the HBS household ids and their derivation to the month and year of the survey filled \n",
    "- (3) dependent_indices -> based on the HBS column indices, this file lists the relevant consumption expense parameters which are predicted \n",
    "- (4) **independent_indices** -> the HBS column indices which define the household socio-economic properties\n",
    "- (5) **target_data** -> Selects the target dataset to predict the results. For most cases, it is the subset of the HBS (for the housing industry, it is the partner dataset 'ABZ', 'SCHL' or 'SM')  \n",
    "- (6) **directory_name** -> based on the industry case, changes the dependent parameters, and income saved by the household (due to which the rebound is supposed to happen) - change the second value in the list. \n",
    "\n",
    "### 0.2. Model parameters\n",
    "- (1) **iter_n** -> no.of iterations of runs\n",
    "- (2) **model_name** -> Random Forest (RF) or ANN (Artificial Neural Network)\n",
    "\n",
    "### 0.3. Analysis parameters\n",
    "- (1) industry change: directory_name with following dependencies \n",
    "    - scenarios, \n",
    "    - partner_name/target dataset,\n",
    "    - idx_column_savings_cons,\n",
    "    - dependent_indices\n",
    "- (2) year change: seasonal_file\n",
    "    - specify which years (2006, 2007, 2008... 2017)\n",
    "- (3) regional change: target_dataset\n",
    "    - specify which regions (DE, IT, FR, ZH)\n",
    "    - specify partner name (ABZ, SCHL, SM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style='color:blue'>USER INPUT NEEDED: chose model settings, methods of preprocessing </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and folder settings\n",
    "directory_name = 'housing' # 'housing' or 'furniture' or 'clothing' or 'energy'        \n",
    "iter_n=1\n",
    "model_name='RF' # 'RF' or 'ANN'\n",
    "\n",
    "## preprocessing methods\n",
    "option_deseason = 'deseasonal' # 'deseasonal' [option 1] or 'month-ind' [option 2]\n",
    "if option_deseason ==  'month-ind':\n",
    "    n_ind = 63\n",
    "    independent_indices='raw_data/independent_month.csv' \n",
    "if option_deseason == 'deseasonal':\n",
    "    n_ind = 39\n",
    "    independent_indices='raw_data/independent.csv' \n",
    "input_normalise = 'no-normalise' #'no-normalise' for not normalising the data or 'normalise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.multioutput as sko\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "from sklearn.metrics import r2_score,mean_squared_error, explained_variance_score\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import brightway2\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd, MultiComparison\n",
    "import statsmodels.api as sm\n",
    "from functools import reduce\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# Additional libraries for neural network implementation \n",
    "\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(2)\n",
    "# from keras import optimizers\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Read the modified files by Nauser et al (2020)\n",
    "# - HBS data (merged raw HBS files) \"HABE_mergerd_2006_2017\" \n",
    "# - tranlsation file 'HABE_Cname_translator.xlsx'\n",
    "# - HBS hhids with the corresponding month of the survey \n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "seasonal_file = 'raw_data/HBS/HABE_merged_2006_2017.csv'\n",
    "seasonal_file_SI = 'raw_data/HBS/HABE_Cname_translator.xlsx'\n",
    "habe_month = 'raw_data/HBS/HABE_date.csv'\n",
    "inf_index_file = 'raw_data/HBS/HABE_inflation_index_all.xlsx'\n",
    "# seasonal_file = 'original_Andi_HBS/habe20092011_hh_prepared_imputed.csv' #based on the years\n",
    "# seasonal_file_SI='original_Andi_HBS/Draft_Paper_8_v11_SupportingInformation.xlsx' \n",
    "# habe_month='original_Andi_HBS/habe_hh_month.csv'\n",
    "\n",
    "\n",
    "## form the databases\n",
    "df_habe = pd.read_csv(seasonal_file, delimiter=',', error_bad_lines=False, encoding='ISO-8859–1')\n",
    "df_habe_month = pd.read_csv(habe_month, delimiter=',', error_bad_lines=False, encoding='ISO-8859–1')\n",
    "inf_index = pd.read_excel(inf_index_file)\n",
    "\n",
    "dependent_indices= 'raw_data/dependent_'+directory_name+'.csv'\n",
    "dependent_indices_pd = pd.read_csv(dependent_indices, delimiter=',', encoding='ISO-8859–1')\n",
    "dependent_indices_pd_name = pd.read_csv(dependent_indices,sep=',')[\"name\"]\n",
    "dependentsize=len(list(dependent_indices_pd_name))\n",
    "\n",
    "independent_indices_pd = pd.read_csv(independent_indices, delimiter=',', encoding='ISO-8859–1')\n",
    "list_independent_columns = pd.read_csv(independent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()\n",
    "list_dependent_columns = pd.read_csv(dependent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add more columns to perform temporal analysis (month_names and time_periods)\n",
    "\n",
    "def label_month (row):\n",
    "    if row['month'] == 1.0 :\n",
    "        return 'January'\n",
    "    if row['month'] == 2.0 :\n",
    "        return 'February'\n",
    "    if row['month'] == 3.0 :\n",
    "        return 'March'\n",
    "    if row['month'] == 4.0 :\n",
    "        return 'April'\n",
    "    if row['month'] == 5.0 :\n",
    "        return 'May'\n",
    "    if row['month'] == 6.0 :\n",
    "        return 'June'\n",
    "    if row['month'] == 7.0 :\n",
    "        return 'July'\n",
    "    if row['month'] == 8.0 :\n",
    "        return 'August'\n",
    "    if row['month'] == 9.0 :\n",
    "        return 'September'\n",
    "    if row['month'] == 10.0 :\n",
    "        return 'October'\n",
    "    if row['month'] == 11.0 :\n",
    "        return 'November'\n",
    "    if row['month'] == 12.0 :\n",
    "        return 'December'\n",
    "    \n",
    "def label_period (row):\n",
    "    if (row[\"year\"] == 2006) or (row[\"year\"] == 2007) or (row[\"year\"] == 2008):\n",
    "        return '1'\n",
    "    if (row[\"year\"] == 2009) or (row[\"year\"] == 2010) or (row[\"year\"] == 2011):\n",
    "        return '2'\n",
    "    if (row[\"year\"] == 2012) or (row[\"year\"] == 2013) or (row[\"year\"] == 2014):\n",
    "        return '3'\n",
    "    if (row[\"year\"] == 2015) or (row[\"year\"] == 2016) or (row[\"year\"] == 2017):\n",
    "        return '4'\n",
    "    \n",
    "df_habe_month['month_name']=df_habe_month.apply(lambda row: label_month(row), axis=1)\n",
    "\n",
    "df_habe_month['period']=df_habe_month.apply(lambda row: label_month(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'color:red'> TODO: update the right values for energy and food industry scenarios, and then merge in the above script </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory_name =='housing':\n",
    "    scenarios = {'baseline_2011':500}\n",
    "    target_data ='ABZ'\n",
    "#     target_data = 'subset-HBS'\n",
    "    idx_column_savings_cons = 'net_rent_and_mortgage_interest_of_principal_residence' #289  \n",
    "    \n",
    "if directory_name == 'furniture':\n",
    "    scenarios = {'refuse_2035':17,'refuse_2050':17.4,'reuse_1_2035':6.9,\n",
    "                           'reuse_1_2050':8.2,'reuse_2_2035':10.2,'reuse_2_2050':9.5}\n",
    "    target_data = 'subset-HBS' \n",
    "    idx_column_savings_cons = 'furniture_and_furnishings,_carpets_and_other_floor_coverings_incl._repairs' #313  \n",
    "    \n",
    "if directory_name == 'clothing':\n",
    "    scenarios = {'sufficiency_2025':76.08,'refuse_2025':5.7075,'share_2025':14.2875,'local_reuse_best_2025':9.13,\n",
    "                         'local_reuse_worst_2025':4.54,'max_local_reuse_best_2025':10.25,'max_local_reuse_worst_2025':6.83}\n",
    "    target_data = 'subset-HBS' \n",
    "    idx_column_savings_cons = 'clothing' #248     \n",
    "    \n",
    "if directory_name == 'energy':  \n",
    "    scenarios = {'efficient_devices':30,'renewable_energy':300}\n",
    "    target_data = 'subset-HBS' \n",
    "    idx_column_savings_cons = 'energy_of_principal_residence' #297 \n",
    "    \n",
    "if directory_name == 'food':  \n",
    "    scenarios = {'avoidable_waste_saving':50}\n",
    "    target_data = 'subset-HBS' \n",
    "    idx_column_savings_cons = 'food_and_non_alcoholic_beverages' #97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to make relevant sector-wise directories\n",
    "\n",
    "def make_pre_directory(outname,directory_name):\n",
    "    outdir = 'preprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname\n",
    "\n",
    "def make_pre_sub_directory(outname,directory_name,sub_dir):\n",
    "    outdir = 'preprocessing/'+directory_name+'/'+sub_dir\n",
    "    outdir1 = 'preprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir1):\n",
    "        os.mkdir(outdir1)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname\n",
    "\n",
    "def make_pre_sub_sub_directory(outname,directory_name,sub_dir,sub_sub_dir):\n",
    "    outdir='preprocessing/'+directory_name+'/'+sub_dir+'/'+sub_sub_dir\n",
    "    outdir1 = 'preprocessing/'+directory_name+'/'+sub_dir\n",
    "    outdir2 = 'preprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir2):\n",
    "        os.mkdir(outdir2)\n",
    "    if not os.path.exists(outdir1):\n",
    "        os.mkdir(outdir1)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing <a id = 'preprocess' ></a>\n",
    "\n",
    "TOC: <a id = 'toc-pre-pre'></a>\n",
    "- <a href = #rename>1.1. Prepare training data</a>\n",
    "- <a href = #deseasonal>1.2. Deseasonalise</a>\n",
    "- <a href = #normal>1.3. Normalize</a>\n",
    "- <a href = #check>1.4. Checks</a>\n",
    "\n",
    "### 1.1. Prepare training data <a id='rename'></a>\n",
    "\n",
    "<a href='#toc-pre'>back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Rename HBS columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_translate = pd.read_excel(seasonal_file_SI, sheet_name='translator', header=3, \n",
    "                              usecols=['habe_code', 'habe_eng_p', 'habe_eng', 'vcode', 'qcode'])\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.strip()\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.replace(' ', '_')\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.replace('-', '_')\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.replace('\"', '')\n",
    "var_translate['habe_eng'] = var_translate['habe_eng'].str.lower()\n",
    "var_translate['habe_code'] = var_translate['habe_code'].str.lower()\n",
    "dict_translate = dict(zip(var_translate['habe_code'], var_translate['habe_eng']))\n",
    "df_habe.rename(columns=dict_translate, inplace=True)\n",
    "dict_translate = dict(zip(var_translate['qcode'], var_translate['habe_eng']))\n",
    "df_habe.rename(columns=dict_translate, inplace=True)\n",
    "df_habe_rename = df_habe.loc[:, ~df_habe.columns.duplicated()]\n",
    "pd.DataFrame.to_csv(df_habe_rename, 'preprocessing/0_habe_rename.csv', sep=',',index=False)\n",
    "\n",
    "######## OLD RENAMING FUNCTIONS ###############\n",
    "\n",
    "# def rename_columns():\n",
    "#     df_rename = df_habe\n",
    "#     df_rename = df_rename.rename(columns = rename_columns_SI('vcode'))\n",
    "#     df_rename = df_rename.rename(columns=rename_columns_SI('qcode'))\n",
    "#     df_rename.columns = df_rename.columns.str.replace(' ', '_')\n",
    "#     df_rename.columns = df_rename.columns.str.replace('-', '_')\n",
    "#     df_rename.columns = df_rename.columns.str.replace('\"', '')\n",
    "#     df_rename.columns = map(str.lower, df_rename.columns)\n",
    "\n",
    "#     pd.DataFrame.to_csv(df_rename, 'preprocessing/0_habe_rename.csv', sep=',',index=False)\n",
    "#     return df_rename\n",
    "\n",
    "\n",
    "# def rename_columns_SI(code):\n",
    "#     df_rename = df_habe\n",
    "#     df_renaming_list = pd.read_excel(seasonal_file_SI, sheet_name='translator', header=3)\n",
    "#     keys = list(df_renaming_list[code])\n",
    "#     values = list(df_renaming_list['habe_eng_p'])\n",
    "#     keys_new = list(df_rename.columns)\n",
    "#     keys_index=[]\n",
    "#     x=list(set(keys) & set(keys_new))\n",
    "#     for y in x:\n",
    "#         keys_index.append(keys.index(y))\n",
    "#     keys_index = sorted(keys_index)\n",
    "#     values_new = []\n",
    "#     keys_new = []\n",
    "#     for h in keys_index:\n",
    "#         keys_new.append(keys[h])\n",
    "#         values_new.append(values[h])\n",
    "#     dictionary = dict(zip(keys_new, values_new))\n",
    "#     return dictionary\n",
    "\n",
    "# df_habe_rename = rename_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Inflation adjustment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_habe_rename = pd.read_csv('preprocessing/0_habe_rename.csv')\n",
    "df_new = pd.merge(df_habe_rename, df_habe_month, on='haushaltid')\n",
    "pd.DataFrame.to_csv(df_new,'preprocessing/0_habe_rename_month.csv', sep=',',index=False)\n",
    "\n",
    "list_var_total = dependent_indices_pd_name.tolist()\n",
    "list_var_total.pop()\n",
    "\n",
    "# monetary variables inflation adjusted\n",
    "list_mon = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "list_year = [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
    "list_var_total = list_var_total + [\"disposable_income\", \"total_expenditures\"]\n",
    "# , 'infrequent_income'] \n",
    "\n",
    "df_inf = df_new\n",
    "\n",
    "for col in list_var_total:\n",
    "    for year in list_year:\n",
    "        for mon in list_mon:\n",
    "            df_inf.loc[(df_inf['year'] == year) & (df_inf['month'] == mon), col] = \\\n",
    "                df_inf.loc[(df_inf['year'] == year) & (df_inf['month'] == mon), col] / \\\n",
    "                inf_index.loc[(inf_index['year'] == year) & (inf_index['month'] == mon), col].values * 100\n",
    "\n",
    "pd.DataFrame.to_csv(df_inf, 'preprocessing/1_habe_inflation.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Adapt the columns (optional - one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_columns(xx,directory_name):\n",
    "    pd_df_saved = df_inf\n",
    "    pd_df_saved.loc[:,'disposable_income'] = pd_df_saved['disposable_income'] - pd_df_saved.loc[:,xx] \n",
    "#     pd_df_saved['total_expenditures'] = pd_df_saved['total_expenditures'] - pd_df_saved.iloc[:,313]\n",
    "    fullname = make_pre_directory('1_habe_rename_new_columns.csv',directory_name)\n",
    "    pd.DataFrame.to_csv(pd_df_saved,fullname, sep=',',index=False)\n",
    "    return pd_df_saved\n",
    "\n",
    "df_habe_rename_saved = new_columns(idx_column_savings_cons,directory_name) # when redefining disposable income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4. Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers():\n",
    "    df_outliers = df_habe_rename_saved  # TODO if using the new definition of disposable income: use the df_habe_rename_saved\n",
    "#     df_outliers = df_outliers[np.abs(stats.zscore(df_outliers['disposable_income']))<10] \n",
    "#     df_outliers = df_outliers[np.abs(stats.zscore(df_outliers['saved_amount_(computed)']))<10]\n",
    "    df_outliers = df_outliers[df_outliers['disposable_income'] >= 0] # simply keep all the 'sensible' disposable incomes\n",
    "    # df_outliers = df_outliers[df_outliers['disposable_income'] <= 14800]  # ADDED CRITERIA FOR REMOVING OUTLIERS OF THE DISP_INCOME\n",
    "    # df_outliers = df_outliers[df_outliers['total_expenditures'] >= 0]  # simply keep all the 'sensible' total_expenses\n",
    "    df_outliers = df_outliers[df_outliers['saved_amount_(computed)'] >= 0]\n",
    "    fullname = make_pre_directory('2_habe_rename_removeoutliers.csv',directory_name)\n",
    "    pd.DataFrame.to_csv(df_outliers, fullname, sep=',', index=False)\n",
    "    return df_outliers\n",
    "\n",
    "df_habe_outliers = remove_outliers()\n",
    "\n",
    "## aggregate the data as per the categories \n",
    "def accumulate_categories_habe(df,new_column,file_name):\n",
    "    list_dependent_columns = pd.read_csv(dependent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()\n",
    "    list_dependent_columns_new = list_dependent_columns\n",
    "    list_dependent_columns_new.append('disposable_income')\n",
    "    list_dependent_columns_new.append(new_column) # Might not always need this\n",
    "    \n",
    "    df = df[list_dependent_columns_new]\n",
    "    df = df.loc[:,~df.columns.duplicated()] #drop duplicates\n",
    "    \n",
    "    df[new_column] = df.iloc[:, [17]]\n",
    "    df['income'] = df.iloc[:, [16]]\n",
    "    df['food'] = df.iloc[:,[0,1,2]].sum(axis=1)\n",
    "    df['misc'] = df.iloc[:,[3,4]].sum(axis=1)\n",
    "    df['housing'] = df.iloc[:, [5, 6]].sum(axis=1)\n",
    "    df['services'] = df.iloc[:, [7,8,9]].sum(axis=1)\n",
    "    df['travel'] = df.iloc[:, [10,11,12, 13, 14]].sum(axis=1)\n",
    "    df['savings'] = df.iloc[:, [15]]    \n",
    "    df = df[['income','food','misc','housing','services','travel','savings',new_column]]\n",
    "    \n",
    "    fullname = make_pre_directory(file_name,directory_name)\n",
    "    pd.DataFrame.to_csv(df,fullname,sep=',',index= False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_outliers = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv')\n",
    "df_habe_accumulate = accumulate_categories_habe(df_outliers,'month_name','2_habe_rename_removeoutliers_aggregated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Deasonalising <a id='deseasonal'></a>\n",
    "- [Option 1] Clustering based on months \n",
    "- [Option 2] Use month and period as independent variable\n",
    "\n",
    "<a href = #toc-pre-pre>back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. [Option 1] Create monthly datasets,  Plots/ Tables / Statistical tests for HABE monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if option_deseason == 'deseasonal' :\n",
    "    \n",
    "    def split_month():\n",
    "        df_new = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv')\n",
    "        df_month = df_new.groupby('month_name')\n",
    "\n",
    "        for i in range(12):\n",
    "            df_new_month=pd.DataFrame(list(df_month)[i][1])\n",
    "            df_new_month['month_name']=df_new_month['month_name'].astype('str')\n",
    "            fullname=make_pre_sub_directory('3_habe_monthly_'+df_new_month.month_name.unique()[0]+'.csv',\n",
    "                                            directory_name,option_deseason)\n",
    "            pd.DataFrame.to_csv(df_new_month,fullname,sep=',', index = False)\n",
    "\n",
    "    split_month()\n",
    "\n",
    "    # Split the accumulated categories per month\n",
    "\n",
    "    def split_month_accumulated():\n",
    "        df_new = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers_aggregated.csv',sep=',')\n",
    "        df_month = df_new.groupby('month_name')\n",
    "        for i in range(12):\n",
    "            df_new_month=pd.DataFrame(list(df_month)[i][1])\n",
    "            df_new_month['month_name']=df_new_month['month_name'].astype('str')\n",
    "            fullname = make_pre_sub_directory('3_habe_monthly_'+df_new_month.month_name.unique()[0]+'_aggregated.csv',\n",
    "                                              directory_name,option_deseason)\n",
    "            pd.DataFrame.to_csv(df_new_month,fullname, sep=',', index = False)\n",
    "\n",
    "    split_month_accumulated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. [Option 1] Making final clusters <a id ='finalclusters'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'color:blue'>USER INPUT NEEDED: edit the cluster-list below</p>\n",
    "\n",
    "<p style = 'color:red'>TODO - join clusters based on the p-values calculated above directly</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('January',)\n",
      "('February', 'March', 'April')\n",
      "('February', 'March', 'April')\n",
      "('February', 'March', 'April')\n",
      "('May', 'June', 'July')\n",
      "('May', 'June', 'July')\n",
      "('May', 'June', 'July')\n",
      "('August', 'September', 'October', 'November')\n",
      "('August', 'September', 'October', 'November')\n",
      "('August', 'September', 'October', 'November')\n",
      "('August', 'September', 'October', 'November')\n",
      "('December',)\n"
     ]
    }
   ],
   "source": [
    "## current clusters are made based on the mean table above\n",
    "\n",
    "if option_deseason == 'deseasonal' :\n",
    "\n",
    "    Cluster_month_lists  = {1:('January',),2:('February','March','April'),3:('May','June','July'),\n",
    "                           4:('August','September','October','November'),5:('December',)}\n",
    "    cluster_number_length = len(Cluster_month_lists)\n",
    "\n",
    "    for key in Cluster_month_lists:\n",
    "        df1=[]\n",
    "        df_sum=[]\n",
    "        for i in range(0,len(Cluster_month_lists[key])):\n",
    "            print(Cluster_month_lists[key])\n",
    "            df=pd.read_csv(make_pre_sub_directory('3_habe_monthly_{}'.format(Cluster_month_lists[key][i])+'.csv',\n",
    "                                                  directory_name,option_deseason))\n",
    "            df_sum.append(df.shape[0])\n",
    "            df1.append(df)\n",
    "        df_cluster = pd.concat(df1)\n",
    "        assert df_cluster.shape[0]==sum(df_sum) # to check if the conacting was done correctly\n",
    "        pd.DataFrame.to_csv(df_cluster,make_pre_sub_directory('4_habe_monthly_cluster_'+str(key)+'.csv',\n",
    "                                                              directory_name,option_deseason),sep=',')\n",
    "\n",
    "# TODO: update this to move to the sub directory of deseaspnal files\n",
    "#     cluster_number_length = len(Cluster_month_lists)\n",
    "#     for i in list(range(1,cluster_number_length+1)):\n",
    "#         accumulate_categories_habe(df,'number_of_persons_per_household','4_habe_monthly_cluster_'+str(i)+'_aggregated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Option 2: Month as independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if option_deseason == 'month-ind' :\n",
    "    cluster_number_length = 1\n",
    "    \n",
    "    # do one-hot encoding for month and year\n",
    "    hbs_all = pd.read_csv('preprocessing/'+directory_name+'/1_habe_rename_new_columns.csv')\n",
    "    month_encoding = pd.get_dummies(hbs_all.month_name, prefix='month')\n",
    "    year_encoding = pd.get_dummies(hbs_all.year, prefix='year')\n",
    "    hbs_all_encoding = pd.concat([hbs_all, month_encoding.reindex(month_encoding.index)], axis=1)\n",
    "    hbs_all_encoding = pd.concat([hbs_all_encoding, year_encoding.reindex(year_encoding.index)], axis=1)\n",
    "    \n",
    "    for key in scenarios:\n",
    "        output_encoding = make_pre_sub_sub_directory('3_habe_for_all_scenarios_encoding.csv',\n",
    "                                                     directory_name,option_deseason,key)\n",
    "        pd.DataFrame.to_csv(hbs_all_encoding,output_encoding,sep=',',index=False)\n",
    "    \n",
    "    month_name = month_encoding.columns.tolist()\n",
    "    year_name = year_encoding.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Normalisation  <a id='normal'></a>\n",
    "\n",
    "<a href='#toc-pre'>back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Normalisation of HBS and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NORMALISATION \n",
    "\n",
    "# if input_normalise == 'normalise':\n",
    "#     def normalise_habe(cluster):\n",
    "#         transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "        \n",
    "#         if option_deseason == 'deseasonal':\n",
    "#             df_deseasonal_file = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "#                                              '/4_habe_monthly_cluster_'+str(cluster)+'.csv', \n",
    "#                                              delimiter=',')\n",
    "#         if option_deseason == 'month-ind':\n",
    "#             df_deseasonal_file = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "#                                              '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "            \n",
    "#         pd_df_new = df_deseasonal_file\n",
    "\n",
    "#         for colsss in list_dependent_columns:\n",
    "#             pd_df_new[[colsss]] = transformer.transform(df_deseasonal_file[[colsss]])\n",
    "\n",
    "#         for colsss in list_independent_columns:\n",
    "#             min_colsss = df_deseasonal_file[[colsss]].quantile([0.01]).values[0]\n",
    "#             max_colsss = df_deseasonal_file[[colsss]].quantile([0.99]).values[0]\n",
    "#             pd_df_new[[colsss]] = (df_deseasonal_file[[colsss]] - min_colsss) / (max_colsss - min_colsss)\n",
    "\n",
    "#         pd_df = pd_df_new[list_independent_columns+['haushaltid']+list_dependent_columns]\n",
    "#         pd_df = pd_df.fillna(0)\n",
    "#         fullname = make_pre_directory('4_habe_deseasonal_'+str(cluster)+'_'+str(option_deseason)+'_normalised.csv',\n",
    "#                                       directory_name)\n",
    "#         pd.DataFrame.to_csv(pd_df,fullname,sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if target_data == 'ABZ':\n",
    "#     if input_normalise =='normalise':\n",
    "#         def normalise_partner(i,key,option_deseason):\n",
    "#             pd_df_partner = pd.read_csv('target_'+target_data+'.csv',delimiter=',')\n",
    "#             df_complete =  pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv',delimiter=',') \n",
    "#             pd_df_partner['disposable_income'] = pd_df_partner['disposable_income'] + i\n",
    "\n",
    "#             for colsss in list_independent_columns:\n",
    "#                 min_colsss = df_complete[[colsss]].quantile([0.01]).values[0]\n",
    "#                 max_colsss = df_complete[[colsss]].quantile([0.99]).values[0]\n",
    "#                 pd_df_partner[[colsss]] = (pd_df_partner[[colsss]] - min_colsss) / (max_colsss - min_colsss)\n",
    "\n",
    "#             # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,30]<=1]\n",
    "#             # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,32]<=1]\n",
    "#             # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,33]>=0] #todo remove rows with normalisation over the range\n",
    "\n",
    "#             fullname = make_pre_sub_sub_directory('5_final_'+ target_data + '_independent_final_'+str(i)+'.csv',\n",
    "#                                               directory_name,option_deseason,key)\n",
    "\n",
    "#             pd.DataFrame.to_csv(pd_df_partner,fullname,sep=',',index=False)\n",
    "#             return pd_df_partner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Preprocessing without normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_normalise == 'no-normalise': \n",
    "    def normalise_habe(cluster):\n",
    "        transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "\n",
    "        if option_deseason == 'deseasonal':\n",
    "            df_deseasonal_file = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "                                             '/4_habe_monthly_cluster_'+str(cluster)+'.csv', \n",
    "                                             delimiter=',')\n",
    "        if option_deseason == 'month-ind':\n",
    "            df_deseasonal_file = pd.read_csv('preprocessing/'+directory_name+ '/' + str(option_deseason) + '/' + str(key) +\n",
    "                                             '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "           \n",
    "        pd_df_new = df_deseasonal_file\n",
    "\n",
    "        pd_df = pd_df_new[list_independent_columns+['haushaltid']+list_dependent_columns]\n",
    "        pd_df = pd_df.fillna(0)\n",
    "        fullname = make_pre_sub_directory('4_habe_deseasonal_'+str(cluster)+'_short.csv',\n",
    "                                      directory_name,option_deseason)\n",
    "        pd.DataFrame.to_csv(pd_df,fullname,sep=',',index=False)\n",
    "        \n",
    "for i in list(range(1,cluster_number_length+1)):\n",
    "    df_normalise_habe_file = normalise_habe(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collecting the independent and dependent datasets\n",
    "\n",
    "def truncate_all(key):\n",
    "    if option_deseason == 'deseasonal':\n",
    "        df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv', \n",
    "                                         delimiter=',', error_bad_lines=False)\n",
    "    if option_deseason == 'month-ind':\n",
    "        df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + str(option_deseason) + '/' + str(key) +\n",
    "                                             '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "    \n",
    "    df_habe_imputed_clustered_d = df_seasonal_normalised[list_dependent_columns]\n",
    "    df_habe_imputed_clustered_i = df_seasonal_normalised[list_independent_columns]\n",
    "    \n",
    "    fullname_d = make_pre_sub_sub_directory('raw_dependent.csv',directory_name,option_deseason,key)\n",
    "    fullname_in = make_pre_sub_sub_directory('raw_independent.csv',directory_name,option_deseason,key)\n",
    "    \n",
    "    pd.DataFrame.to_csv(df_habe_imputed_clustered_d,fullname_d,sep=',',index=False)\n",
    "    pd.DataFrame.to_csv(df_habe_imputed_clustered_i,fullname_in,sep=',',index=False)\n",
    "\n",
    "for key in scenarios:\n",
    "    truncate_all(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NORMALISATION \n",
    "\n",
    "if target_data == 'subset-HBS':\n",
    "    def normalise_partner(i,key,option_deseason):\n",
    "        N = 300 # TODO pass this as an argument when chosing subset of HBS\n",
    "        pd_df_partner = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/raw_independent.csv', \n",
    "                                    delimiter=',', error_bad_lines=False)\n",
    "        pd_df_partner = pd_df_partner.sample(frac=0.4, replace=True, random_state=1)\n",
    "        pd_df_partner['disposable_income'] = pd_df_partner['disposable_income']+i\n",
    "        fullname = make_pre_sub_sub_directory('5_final_'+ target_data + '_independent_final_'+str(i)+'.csv',\n",
    "                                          directory_name,option_deseason,key)\n",
    "\n",
    "        pd.DataFrame.to_csv(pd_df_partner,fullname,sep=',',index=False)\n",
    "        return pd_df_partner\n",
    "    \n",
    "\n",
    "if target_data == 'ABZ':\n",
    "    if input_normalise =='no-normalise':\n",
    "        def normalise_partner(i,key,option_deseason):\n",
    "            pd_df_partner = pd.read_csv('raw_data/target_'+target_data+'.csv',delimiter=',')\n",
    "            df_complete =  pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv',delimiter=',') \n",
    "            pd_df_partner['disposable_income'] = pd_df_partner['disposable_income'] - i\n",
    "\n",
    "            # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,30]<=1]\n",
    "            # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,32]<=1]\n",
    "            # pd_df_partner = pd_df_partner[pd_df_partner.iloc[:,33]>=0] #todo remove rows with normalisation over the range\n",
    "\n",
    "            fullname = make_pre_sub_sub_directory('5_final_'+ target_data + '_independent_final_'+str(i)+'.csv',\n",
    "                                              directory_name,option_deseason,key)\n",
    "\n",
    "            pd.DataFrame.to_csv(pd_df_partner,fullname,sep=',',index=False)\n",
    "            return pd_df_partner\n",
    "\n",
    "for key in scenarios:\n",
    "    list_incomechange=[0,scenarios[key]]\n",
    "    for i in list_incomechange:\n",
    "        df_normalise_partner_file = normalise_partner(i,key,option_deseason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Checks<a id='check'></a>\n",
    "\n",
    "<a href='#toc-pre'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if input_normalise =='normalise':\n",
    "#     def truncate(cluster_number):\n",
    "#         if option_deseason == 'deseasonal':\n",
    "#             df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "#                                                  '/4_habe_deseasonal_'+str(cluster_number)+'_normalised.csv', \n",
    "#                                                  delimiter=',', error_bad_lines=False)\n",
    "#         if option_deseason == 'month-ind':\n",
    "#             df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + str(option_deseason) + '/' + str(key) +\n",
    "#                                              '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "#         df_habe_imputed_clustered_d = df_seasonal_normalised[list_dependent_columns]\n",
    "#         df_habe_imputed_clustered_dl = np.expm1(df_habe_imputed_clustered_d)\n",
    "#         df_habe_imputed_clustered_i = df_seasonal_normalised[list_independent_columns]\n",
    "        \n",
    "#         fullname_dl = make_pre_sub_sub_directory('raw_dependent_old_'+str(cluster_number)+'.csv',directory_name,\n",
    "#                                                  'checks',option_deseason)\n",
    "#         fullname_d = make_pre_sub_sub_directory('raw_dependent_'+str(cluster_number)+'.csv',directory_name,\n",
    "#                                                 'checks',option_deseason)\n",
    "#         fullname_in = make_pre_sub_sub_directory('raw_independent_'+str(cluster_number)+'.csv',directory_name,\n",
    "#                                                  'checks',option_deseason)\n",
    "#         pd.DataFrame.to_csv(df_habe_imputed_clustered_dl,fullname_dl,sep=',',index=False)\n",
    "#         pd.DataFrame.to_csv(df_habe_imputed_clustered_d,fullname_d,sep=',',index=False)\n",
    "#         pd.DataFrame.to_csv(df_habe_imputed_clustered_i,fullname_in,sep=',',index=False)\n",
    "    \n",
    "if input_normalise =='no-normalise':\n",
    "    def truncate(cluster_number):\n",
    "        if option_deseason == 'deseasonal':\n",
    "            df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + option_deseason +\n",
    "                                                 '/4_habe_deseasonal_'+str(cluster_number)+'_short.csv', \n",
    "                                                 delimiter=',', error_bad_lines=False)\n",
    "        if option_deseason == 'month-ind':\n",
    "            df_seasonal_normalised = pd.read_csv('preprocessing/'+directory_name+ '/' + str(option_deseason) + '/' + str(key) +\n",
    "                                             '/3_habe_for_all_scenarios_encoding.csv',delimiter=',')\n",
    "        df_habe_imputed_clustered_d = df_seasonal_normalised[list_dependent_columns]\n",
    "        df_habe_imputed_clustered_i = df_seasonal_normalised[list_independent_columns]\n",
    "        \n",
    "        fullname_d = make_pre_sub_sub_directory('raw_dependent_'+str(cluster_number)+'.csv',directory_name,\n",
    "                                            'checks',option_deseason)\n",
    "        fullname_in = make_pre_sub_sub_directory('raw_independent_'+str(cluster_number)+'.csv',directory_name,\n",
    "                                             'checks',option_deseason)\n",
    "        pd.DataFrame.to_csv(df_habe_imputed_clustered_d,fullname_d,sep=',',index=False)\n",
    "        pd.DataFrame.to_csv(df_habe_imputed_clustered_i,fullname_in,sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(1,cluster_number_length+1)):\n",
    "    truncate(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MODEL <a id = \"model\"></a>\n",
    "    \n",
    "<a href = \"#toc\">back</a>\n",
    "\n",
    "TOC:<a id ='toc-model'></a>\n",
    "- <a href = \"#prep\"> 2.1. Prepare train-test-target datasets</a>\n",
    "- <a href = \"#predict\"> 2.2. Prediction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Prepare train-test-target datasets <a id ='prep'></a>\n",
    "\n",
    "<a href=#toc-model>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_haushalts(values,id_ix=0):\n",
    "    haushalts = dict()\n",
    "    haushalt_ids = np.unique(values[:,id_ix])\n",
    "    for haushalt_id in haushalt_ids:\n",
    "        selection = values[:, id_ix] == haushalt_id\n",
    "        haushalts[haushalt_id] = values[selection]\n",
    "    return haushalts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(haushalts,length_training,month_name,row_in_chunk):\n",
    "    train, test = list(), list()\n",
    "    cut_point = int(0.8*length_training)  # 0.9*9754 # declare cut_point as per the size of the imputed database #TODO check if this is too less\n",
    "    print('Month/cluster and cut_point',month_name, cut_point)\n",
    "    for k,rows in haushalts.items():\n",
    "        train_rows = rows[rows[:,row_in_chunk] < cut_point, :]\n",
    "        test_rows = rows[rows[:,row_in_chunk] > cut_point, :]\n",
    "        train.append(train_rows[:, :])\n",
    "        test.append(test_rows[:, :])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### NORMALISATION\n",
    "\n",
    "# if input_normalise =='normalise':\n",
    "\n",
    "#     def df_habe_train_test(df,month_name,length_training):\n",
    "#         df=df.assign(id_split = list(range(df.shape[0])))\n",
    "#         train, test = split_train_test(to_haushalts(df.values),length_training,month_name,row_in_chunk=df.shape[1]-1)\n",
    "\n",
    "#         train_rows = np.array([row for rows in train for row in rows])\n",
    "#         test_rows = np.array([row for rows in test for row in rows])\n",
    "\n",
    "#         independent = list(range(0,independent_indices_pd.shape[0]))\n",
    "#         dependent =  list(range(independent_indices_pd.shape[0]+1,\n",
    "#                                 independent_indices_pd.shape[0]+dependent_indices_pd.shape[0]+1))\n",
    "\n",
    "#         trained_independent = train_rows[:, independent]\n",
    "#         trained_dependent = train_rows[:, dependent]\n",
    "#         test_independent = test_rows[:, independent]\n",
    "#         test_dependent = test_rows[:, dependent]\n",
    "\n",
    "#         ## OPTIONAL lines FOR CHECK - comment if not needed\n",
    "#         np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/trained_dependent_nonexp.csv', \n",
    "#                    trained_dependent, delimiter=',')    \n",
    "#         np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/trained_dependent.csv', \n",
    "#                    np.expm1(trained_dependent),delimiter=',')\n",
    "#         np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/trained_independent.csv', \n",
    "#                    trained_independent, delimiter=',')\n",
    "#         np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/test_dependent.csv', \n",
    "#                    np.expm1(test_dependent), delimiter=',')\n",
    "#         np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/test_independent.csv', \n",
    "#                    test_independent, delimiter=',')\n",
    "\n",
    "#         return trained_independent,trained_dependent,test_independent,test_dependent\n",
    "\n",
    "#     def df_partner_test(y):\n",
    "#         df_partner = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/5_final_' + target_data + \n",
    "#                                  '_independent_final_' + str(y) + '.csv',delimiter=',')\n",
    "#         length_training = df_partner.shape[0]\n",
    "#         train_partner, test_partner = split_train_test(to_haushalts(df_partner.values),length_training,month_name,1) \n",
    "#         train_rows_partner = np.array([row for rows in train_partner for row in rows])\n",
    "#         new_independent = list(range(0, n_ind)) # number of columns of the independent parameters\n",
    "#         train_partner_independent = train_rows_partner[:, new_independent]\n",
    "\n",
    "#         ### Optional lines for CHECK - comment if not needed\n",
    "#         np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/train_partner_independent_' + model_name + '_' + str(y) + '.csv',\n",
    "#                    train_partner_independent, delimiter=',')\n",
    "\n",
    "#         return train_partner_independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## form the train test datasets \n",
    "\n",
    "# NO-NORMALISATION\n",
    "if input_normalise =='no-normalise':\n",
    "    def df_habe_train_test(df,month_name,length_training):\n",
    "        df=df.assign(id_split = list(range(df.shape[0])))\n",
    "        train, test = split_train_test(to_haushalts(df.values),length_training,month_name,row_in_chunk=df.shape[1]-1)\n",
    "\n",
    "        train_rows = np.array([row for rows in train for row in rows])\n",
    "        test_rows = np.array([row for rows in test for row in rows])\n",
    "\n",
    "        independent = list(range(0,independent_indices_pd.shape[0]))\n",
    "        dependent =  list(range(independent_indices_pd.shape[0]+1,\n",
    "                                independent_indices_pd.shape[0]+dependent_indices_pd.shape[0]+1))\n",
    "\n",
    "        trained_independent = train_rows[:, independent]\n",
    "        trained_dependent = train_rows[:, dependent]\n",
    "        test_independent = test_rows[:, independent]\n",
    "        test_dependent = test_rows[:, dependent]\n",
    "\n",
    "        ## OPTIONAL lines FOR CHECK - comment if not needed\n",
    "        # np.savetxt('raw/checks/trained_dependent_nonexp_'+str(month_name)+'.csv', trained_dependent, delimiter=',')    \n",
    "        # np.savetxt('raw/checks/trained_independent_nonexp_'+str(month_name)+'.csv', trained_independent, delimiter=',')\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/test_dependent_'+str(month_name)+'.csv', \n",
    "                   test_dependent,delimiter=',')    \n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/test_independent_'+str(month_name)+'.csv', \n",
    "                   test_independent, delimiter=',')\n",
    "\n",
    "        return trained_independent,trained_dependent,test_independent,test_dependent\n",
    "    \n",
    "    def df_partner_test(y):\n",
    "        df_partner = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/5_final_' + target_data + \n",
    "                                 '_independent_final_' + str(y) + '.csv', delimiter=',')\n",
    "        length_training = df_partner.shape[0]\n",
    "        train_partner, test_partner = split_train_test(to_haushalts(df_partner.values),\n",
    "                                                       length_training,cluster_number,1) \n",
    "        train_rows_partner = np.array([row for rows in train_partner for row in rows])\n",
    "        new_independent = list(range(0, n_ind))\n",
    "        train_partner_independent = train_rows_partner[:, new_independent]\n",
    "\n",
    "        ### Optional lines for CHECK - comment if not needed\n",
    "        np.savetxt('preprocessing/'+directory_name+'/checks/'+option_deseason+'/train_partner_independent_' + \n",
    "                   model_name + '_' + str(y) + '.csv', train_partner_independent, delimiter=',')\n",
    "\n",
    "        return train_partner_independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_post_directory(outname,directory_name):\n",
    "    outdir = 'postprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname\n",
    "\n",
    "def make_post_sub_directory(outname,directory_name,sub_dir):\n",
    "    outdir_1='postprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir_1):\n",
    "        os.mkdir(outdir_1)\n",
    "    outdir = 'postprocessing/'+directory_name+'/'+sub_dir\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "    return fullname\n",
    "\n",
    "def make_post_sub_sub_directory(outname,directory_name,sub_dir,sub_sub_dir):\n",
    "    outdir_1='postprocessing/'+directory_name\n",
    "    if not os.path.exists(outdir_1):\n",
    "        os.mkdir(outdir_1)\n",
    "    outdir = 'postprocessing/'+directory_name+'/'+sub_dir\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    outdir_2='postprocessing/'+directory_name+'/'+sub_dir+'/'+sub_sub_dir\n",
    "    if not os.path.exists(outdir_2):\n",
    "        os.mkdir(outdir_2)\n",
    "    fullname = os.path.join(outdir_2, outname)\n",
    "    return fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR NO NORMALISATION AND TEST DATA \n",
    "\n",
    "# def df_test(y,cluster_number):\n",
    "#     pd_df_partner = pd.read_csv('raw/checks/trained_independent_'+str(cluster_number)+'.csv', delimiter=',', header = None)\n",
    "#     pd_df_partner.iloc[:,-1] = pd_df_partner.iloc[:,-1] + y\n",
    "\n",
    "#     pd.DataFrame.to_csv(pd_df_partner, 'raw/checks/5_trained_independent_'+str(cluster_number)+'_'+str(y)+'.csv', \n",
    "#                         sep=',',index=False)\n",
    "#     return pd_df_partner\n",
    "\n",
    "# def df_stratified_test(y):\n",
    "#     pd_df_partner = pd.read_csv('raw/checks/5_setstratified_independent_1_'+str(y)+'.csv', delimiter=',')\n",
    "#     return pd_df_partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using Neural Networks\n",
    "\n",
    "# def ANN():\n",
    "#     nn = Sequential()\n",
    "#     nn.add(Dense(39,kernel_initializer='normal',activation=\"relu\",input_shape=(39,)))\n",
    "#     nn.add(Dense(50,kernel_initializer='normal',activation=\"relu\"))\n",
    "#     nn.add(Dense(100,kernel_initializer='normal',activation=\"relu\"))\n",
    "#     nn.add(Dense(100,kernel_initializer='normal',activation=\"relu\") )\n",
    "#     # nn.add(Dense(100,kernel_initializer='normal',activation=\"relu\"))\n",
    "#     # nn.add(Dense(100,kernel_initializer='normal',activation=\"relu\"))\n",
    "#     nn.add(Dense(dependentsize,kernel_initializer='normal')) #,kernel_constraint=min_max_norm(min_value=0.01,max_value=0.05)))\n",
    "#     sgd = optimizers.SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#     nn.compile(optimizer=sgd, loss='mean_squared_error', metrics=['accuracy'])\n",
    "#     return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Clustered Prediction <a id='predict'></a>\n",
    "\n",
    "<a href='#toc-model'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NORMALISATION\n",
    "\n",
    "# if input_normalise =='normalise':\n",
    "\n",
    "#     def fit_predict_cluster(i,y,cluster_number,key):\n",
    "#         df = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+\n",
    "#                          '/4_habe_deseasonal_'+str(cluster_number)+'_normalised.csv',\n",
    "#                          delimiter=',',error_bad_lines=False, encoding='ISO-8859–1')\n",
    "#         length_training = df.shape[0]\n",
    "#         trained_independent, trained_dependent, test_independent, test_dependent = df_habe_train_test(df,\n",
    "#                                                                                                       str(cluster_number),\n",
    "#                                                                                                       length_training)\n",
    "#         train_partner_independent = df_partner_test(y)\n",
    "        \n",
    "#         if model_name == 'ANN':\n",
    "#             estimator = KerasRegressor(build_fn=ANN)\n",
    "#             estimator.fit(trained_independent, trained_dependent, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "#             ### PREDICTION FROM HERE\n",
    "#             prediction_nn = estimator.predict(train_partner_independent)\n",
    "#             prediction_nn_denormalised = np.expm1(prediction_nn)\n",
    "#             fullname = make_post_sub_sub_directory('predicted_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "#                        + '_' + str(cluster_number) + '.csv',directory_name,option_deseason,key)\n",
    "#             np.savetxt(fullname, prediction_nn_denormalised, delimiter=',')\n",
    "\n",
    "#             ### TEST PREDICTION\n",
    "#             prediction_nn_test = estimator.predict(test_independent)\n",
    "#             prediction_nn_test_denormalised = np.expm1(prediction_nn_test)\n",
    "#             fullname = make_post_sub_sub_directory('predicted_test' + model_name + '_' + str(y) + '_' + str(i) \n",
    "#                        + '_' + str(cluster_number) + '.csv',directory_name,option_deseason,key)\n",
    "#             np.savetxt(fullname, prediction_nn_test_denormalised, delimiter=',')\n",
    "\n",
    "#             ### CROSS VALIDATION FROM HERE\n",
    "#             kfold = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "#             results1 = cross_val_score(estimator, test_independent, test_dependent, cv=kfold)\n",
    "#             print(\"Results_test: %.2f (%.2f)\" % (results1.mean(), results1.std()))\n",
    "\n",
    "#         if model_name == 'RF':\n",
    "#             estimator = sko.MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_features=n_ind, random_state=30))\n",
    "#             estimator.fit(trained_independent, trained_dependent)\n",
    "\n",
    "#             ### PREDICTION FROM HERE\n",
    "#             prediction_nn = estimator.predict(train_partner_independent)\n",
    "#             results0 = estimator.oob_score\n",
    "#             prediction_nn_denormalised = np.expm1(prediction_nn)\n",
    "#             fullname = make_post_sub_sub_directory('predicted_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "#                        + '_' + str(cluster_number) + '.csv',directory_name,option_deseason,key)\n",
    "#             np.savetxt(fullname, prediction_nn_denormalised, delimiter=',')\n",
    "\n",
    "#              ### TEST PREDICTION\n",
    "#             prediction_nn_test = estimator.predict(test_independent)\n",
    "#             prediction_nn_test_denormalised = np.expm1(prediction_nn_test)\n",
    "#             fullname = make_post_sub_sub_directory('predicted_test' + model_name + '_' + str(y) + '_' + str(i) \n",
    "#                        + '_' + str(cluster_number) + '.csv',directory_name,option_deseason,key)\n",
    "#             np.savetxt(fullname, prediction_nn_test_denormalised, delimiter=',')        \n",
    "\n",
    "#             #### CROSS VALIDATION FROM HERE\n",
    "#             kfold = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "#             # results0 = estimator.oob_score\n",
    "#             # results1 = cross_val_score(estimator, test_independent, test_dependent, cv=kfold)\n",
    "#             results2 = r2_score(test_dependent,prediction_nn_test)\n",
    "#             results3 = mean_squared_error(test_dependent,prediction_nn_test)\n",
    "#             results4 = explained_variance_score(test_dependent,prediction_nn_test)\n",
    "#             # print(\"cross_val_score: %.2f (%.2f)\" % (results1.mean(), results1.std()))\n",
    "#             # print(\"oob_r2_score: %.2f \" % results0)\n",
    "#             print(\"r2_score: %.2f \" % results2)\n",
    "#             print(\"mean_squared_error: %.2f \" % results3)\n",
    "#             print(\"explained_variance_score: %.2f \" % results4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR NO NORMALISATION\n",
    "\n",
    "if input_normalise =='no-normalise':\n",
    "    def fit_predict_cluster(i,y,cluster_number,key):\n",
    "        df_non_normalised = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/4_habe_deseasonal_'+\n",
    "                                        str(cluster_number)+ '_short.csv', delimiter=',',\n",
    "                                        error_bad_lines=False, encoding='ISO-8859–1')\n",
    "        length_training = df_non_normalised.shape[0]\n",
    "        print(length_training)\n",
    "        trained_independent, trained_dependent, test_independent, test_dependent = df_habe_train_test(df_non_normalised,\n",
    "                                                                                                      str(cluster_number),\n",
    "                                                                                                      length_training)\n",
    "        train_partner_independent = df_partner_test(y)\n",
    "        \n",
    "        ### Additional for the HBS test data subset\n",
    "        # test_new_independent = df_test(y,1) # chosing just one cluster here\n",
    "        # sratified_independent = df_stratified_test(y)\n",
    "    \n",
    "        if model_name == 'ANN':\n",
    "            estimator = KerasRegressor(build_fn=ANN)\n",
    "            estimator.fit(trained_independent, trained_dependent, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "            ### PREDICTION FROM HERE\n",
    "            prediction_nn = estimator.predict(train_partner_independent)\n",
    "            fullname = make_post_sub_sub_directory('predicted_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) +'.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn, delimiter=',')\n",
    "\n",
    "            ### TEST PREDICTION\n",
    "            prediction_nn_test = estimator.predict(test_independent)\n",
    "            fullname = make_post_sub_sub_directory('predicted_test_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) +'.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn_test, delimiter=',')\n",
    "\n",
    "            ### CROSS VALIDATION FROM HERE\n",
    "            kfold = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "            results1 = cross_val_score(estimator, test_independent, test_dependent, cv=kfold)\n",
    "            print(\"Results_test: %.2f (%.2f)\" % (results1.mean(), results1.std()))\n",
    "\n",
    "        if model_name == 'RF':\n",
    "            estimator = sko.MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_features=n_ind, random_state=30))\n",
    "            estimator.fit(trained_independent, trained_dependent)\n",
    "            \n",
    "            ### FEATURE IMPORTANCE\n",
    "            rf = RandomForestRegressor()\n",
    "            rf.fit(trained_independent, trained_dependent)\n",
    "            FI = rf.feature_importances_\n",
    "            list_independent_columns = pd.read_csv(independent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()\n",
    "            independent_columns = pd.DataFrame(list_independent_columns)\n",
    "            FI_names = pd.DataFrame(FI)\n",
    "            FI_names = pd.concat([independent_columns, FI_names], axis=1)\n",
    "            FI_names.columns = ['independent_variables', 'FI_score']\n",
    "            pd.DataFrame.to_csv(FI_names,'preprocessing/'+directory_name+'/8_habe_feature_importance'+ '_' +\n",
    "                                str(y) + '_' + str(i) + '_' + str(cluster_number) +'.csv', sep=',',index= False)\n",
    "            FI_names_sorted = FI_names.sort_values('FI_score', ascending = False)\n",
    "#             print(FI_names_sorted)\n",
    "\n",
    "            ### PREDICTION FROM HERE\n",
    "            prediction_nn = estimator.predict(train_partner_independent)\n",
    "            fullname = make_post_sub_sub_directory('predicted_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) +'.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn, delimiter=',')\n",
    "\n",
    "             ### TEST PREDICTION\n",
    "            prediction_nn_test = estimator.predict(test_independent)\n",
    "            fullname = make_post_sub_sub_directory('predicted_test_' + model_name + '_' + str(y) + '_' + str(i) \n",
    "                       + '_' + str(cluster_number) +'.csv',directory_name,option_deseason,key)\n",
    "            np.savetxt(fullname, prediction_nn_test, delimiter=',')     \n",
    "\n",
    "            #### CROSS VALIDATION FROM HERE\n",
    "            kfold = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "            \n",
    "            for i in range(16):\n",
    "                column_predict = pd.DataFrame(test_dependent).iloc[:,i]\n",
    "                model = sm.OLS(column_predict, test_independent).fit() \n",
    "                print(i)\n",
    "                print('standard error=',model.bse) \n",
    "            \n",
    "            # results0 = estimator.oob_score\n",
    "            # results1 = cross_val_score(estimator, test_independent, test_dependent, cv=kfold)\n",
    "#             results2 = r2_score(test_dependent,prediction_nn_test)\n",
    "#             results3 = mean_squared_error(test_dependent,prediction_nn_test)\n",
    "#             results4 = explained_variance_score(test_dependent,prediction_nn_test)\n",
    "            # print(\"cross_val_score: %.2f (%.2f)\" % (results1.mean(), results1.std()))\n",
    "            # print(\"oob_r2_score: %.2f \" % results0)\n",
    "            print(\"r2_score: %.2f \" % results2)\n",
    "            print(\"mean_squared_error: %.2f \" % results3)\n",
    "            print(\"explained_variance_score: %.2f \" % results4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2301\n",
      "Month/cluster and cut_point 1 1840\n",
      "Month/cluster and cut_point 1 1199\n",
      "0\n",
      "standard error= x1     40.490810\n",
      "x2     62.180617\n",
      "x3     56.538947\n",
      "x4     59.404511\n",
      "x5     61.127747\n",
      "x6     63.129861\n",
      "x7     59.727916\n",
      "x8     57.480385\n",
      "x9     10.596318\n",
      "x10     8.655813\n",
      "x11    27.756329\n",
      "x12    44.666962\n",
      "x13    22.514507\n",
      "x14    46.116155\n",
      "x15    31.469952\n",
      "x16    45.371581\n",
      "x17    40.587932\n",
      "x18    44.957662\n",
      "x19    42.205714\n",
      "x20    40.076987\n",
      "x21    51.184074\n",
      "x22    70.868329\n",
      "x23    39.386659\n",
      "x24    34.649143\n",
      "x25    44.974733\n",
      "x26    42.571748\n",
      "x27    41.757918\n",
      "x28    42.678597\n",
      "x29    40.474273\n",
      "x30    52.930873\n",
      "x31    75.118096\n",
      "x32    16.421303\n",
      "x33    32.867710\n",
      "x34    12.805623\n",
      "x35    34.176171\n",
      "x36    28.871578\n",
      "x37    20.073164\n",
      "x38    44.995352\n",
      "x39     0.003324\n",
      "dtype: float64\n",
      "1\n",
      "standard error= x1     26.571955\n",
      "x2     40.805816\n",
      "x3     37.103489\n",
      "x4     38.984006\n",
      "x5     40.114874\n",
      "x6     41.428754\n",
      "x7     39.196239\n",
      "x8     37.721305\n",
      "x9      6.953798\n",
      "x10     5.680347\n",
      "x11    18.214996\n",
      "x12    29.312540\n",
      "x13    14.775068\n",
      "x14    30.263568\n",
      "x15    20.652048\n",
      "x16    29.774944\n",
      "x17    26.635691\n",
      "x18    29.503311\n",
      "x19    27.697355\n",
      "x20    26.300385\n",
      "x21    33.589373\n",
      "x22    46.507097\n",
      "x23    25.847360\n",
      "x24    22.738381\n",
      "x25    29.514514\n",
      "x26    27.937564\n",
      "x27    27.403490\n",
      "x28    28.007682\n",
      "x29    26.561102\n",
      "x30    34.735703\n",
      "x31    49.295992\n",
      "x32    10.776424\n",
      "x33    21.569322\n",
      "x34     8.403646\n",
      "x35    22.427995\n",
      "x36    18.946874\n",
      "x37    13.172945\n",
      "x38    29.528045\n",
      "x39     0.002182\n",
      "dtype: float64\n",
      "2\n",
      "standard error= x1      71.419298\n",
      "x2     109.676640\n",
      "x3      99.725638\n",
      "x4     104.780035\n",
      "x5     107.819547\n",
      "x6     111.350954\n",
      "x7     105.350469\n",
      "x8     101.386184\n",
      "x9      18.690207\n",
      "x10     15.267466\n",
      "x11     48.957715\n",
      "x12     78.785361\n",
      "x13     39.711982\n",
      "x14     81.341505\n",
      "x15     55.507951\n",
      "x16     80.028195\n",
      "x17     71.590605\n",
      "x18     79.298109\n",
      "x19     74.444114\n",
      "x20     70.689381\n",
      "x21     90.280502\n",
      "x22    125.000371\n",
      "x23     69.471753\n",
      "x24     61.115534\n",
      "x25     79.328219\n",
      "x26     75.089740\n",
      "x27     73.654274\n",
      "x28     75.278203\n",
      "x29     71.390129\n",
      "x30     93.361574\n",
      "x31    132.496280\n",
      "x32     28.964546\n",
      "x33     57.973371\n",
      "x34     22.587066\n",
      "x35     60.281288\n",
      "x36     50.924836\n",
      "x37     35.405843\n",
      "x38     79.364587\n",
      "x39      0.005864\n",
      "dtype: float64\n",
      "3\n",
      "standard error= x1     51.326460\n",
      "x2     78.820625\n",
      "x3     71.669200\n",
      "x4     75.301612\n",
      "x5     77.485999\n",
      "x6     80.023893\n",
      "x7     75.711562\n",
      "x8     72.862574\n",
      "x9     13.431974\n",
      "x10    10.972174\n",
      "x11    35.184135\n",
      "x12    56.620182\n",
      "x13    28.539561\n",
      "x14    58.457191\n",
      "x15    39.891552\n",
      "x16    57.513362\n",
      "x17    51.449572\n",
      "x18    56.988676\n",
      "x19    53.500285\n",
      "x20    50.801895\n",
      "x21    64.881323\n",
      "x22    89.833234\n",
      "x23    49.926830\n",
      "x24    43.921519\n",
      "x25    57.010315\n",
      "x26    53.964273\n",
      "x27    52.932656\n",
      "x28    54.099715\n",
      "x29    51.305497\n",
      "x30    67.095578\n",
      "x31    95.220272\n",
      "x32    20.815769\n",
      "x33    41.663360\n",
      "x34    16.232505\n",
      "x35    43.321976\n",
      "x36    36.597833\n",
      "x37    25.444896\n",
      "x38    57.036451\n",
      "x39     0.004214\n",
      "dtype: float64\n",
      "4\n",
      "standard error= x1     50.944612\n",
      "x2     78.234232\n",
      "x3     71.136011\n",
      "x4     74.741399\n",
      "x5     76.909536\n",
      "x6     79.428549\n",
      "x7     75.148300\n",
      "x8     72.320507\n",
      "x9     13.332046\n",
      "x10    10.890546\n",
      "x11    34.922379\n",
      "x12    56.198952\n",
      "x13    28.327239\n",
      "x14    58.022293\n",
      "x15    39.594775\n",
      "x16    57.085487\n",
      "x17    51.066809\n",
      "x18    56.564704\n",
      "x19    53.102265\n",
      "x20    50.423950\n",
      "x21    64.398633\n",
      "x22    89.164912\n",
      "x23    49.555395\n",
      "x24    43.594761\n",
      "x25    56.586182\n",
      "x26    53.562802\n",
      "x27    52.538859\n",
      "x28    53.697236\n",
      "x29    50.923806\n",
      "x30    66.596415\n",
      "x31    94.511873\n",
      "x32    20.660908\n",
      "x33    41.353402\n",
      "x34    16.111742\n",
      "x35    42.999678\n",
      "x36    36.325560\n",
      "x37    25.255596\n",
      "x38    56.612124\n",
      "x39     0.004183\n",
      "dtype: float64\n",
      "5\n",
      "standard error= x1     110.234133\n",
      "x2     169.283508\n",
      "x3     153.924354\n",
      "x4     161.725705\n",
      "x5     166.417126\n",
      "x6     171.867776\n",
      "x7     162.606157\n",
      "x8     156.487369\n",
      "x9      28.847928\n",
      "x10     23.565002\n",
      "x11     75.565168\n",
      "x12    121.603491\n",
      "x13     61.294580\n",
      "x14    125.548844\n",
      "x15     85.675315\n",
      "x16    123.521778\n",
      "x17    110.498542\n",
      "x18    122.394906\n",
      "x19    114.902870\n",
      "x20    109.107522\n",
      "x21    139.345991\n",
      "x22    192.935354\n",
      "x23    107.228139\n",
      "x24     94.330498\n",
      "x25    122.441380\n",
      "x26    115.899380\n",
      "x27    113.683769\n",
      "x28    116.190269\n",
      "x29    110.189112\n",
      "x30    144.101559\n",
      "x31    204.505126\n",
      "x32     44.706147\n",
      "x33     89.480637\n",
      "x34     34.862645\n",
      "x35     93.042856\n",
      "x36     78.601376\n",
      "x37     54.648149\n",
      "x38    122.497514\n",
      "x39      0.009050\n",
      "dtype: float64\n",
      "6\n",
      "standard error= x1      63.062036\n",
      "x2      96.842624\n",
      "x3      88.056057\n",
      "x4      92.519004\n",
      "x5      95.202842\n",
      "x6      98.321015\n",
      "x7      93.022688\n",
      "x8      89.522291\n",
      "x9      16.503138\n",
      "x10     13.480915\n",
      "x11     43.228837\n",
      "x12     69.566145\n",
      "x13     35.065010\n",
      "x14     71.823178\n",
      "x15     49.012585\n",
      "x16     70.663547\n",
      "x17     63.213297\n",
      "x18     70.018893\n",
      "x19     65.732897\n",
      "x20     62.417532\n",
      "x21     79.716161\n",
      "x22    110.373220\n",
      "x23     61.342386\n",
      "x24     53.963986\n",
      "x25     70.045480\n",
      "x26     66.302974\n",
      "x27     65.035482\n",
      "x28     66.469384\n",
      "x29     63.036280\n",
      "x30     82.436696\n",
      "x31    116.991981\n",
      "x32     25.575206\n",
      "x33     51.189509\n",
      "x34     19.943998\n",
      "x35     53.227361\n",
      "x36     44.965772\n",
      "x37     31.262763\n",
      "x38     70.077592\n",
      "x39      0.005178\n",
      "dtype: float64\n",
      "7\n",
      "standard error= x1     14.980344\n",
      "x2     23.004900\n",
      "x3     20.917657\n",
      "x4     21.977827\n",
      "x5     22.615371\n",
      "x6     23.356091\n",
      "x7     22.097477\n",
      "x8     21.265960\n",
      "x9      3.920309\n",
      "x10     3.202382\n",
      "x11    10.268981\n",
      "x12    16.525391\n",
      "x13     8.329670\n",
      "x14    17.061548\n",
      "x15    11.642907\n",
      "x16    16.786078\n",
      "x17    15.016276\n",
      "x18    16.632941\n",
      "x19    15.614806\n",
      "x20    14.827243\n",
      "x21    18.936520\n",
      "x22    26.219084\n",
      "x23    14.571843\n",
      "x24    12.819109\n",
      "x25    16.639257\n",
      "x26    15.750227\n",
      "x27    15.449135\n",
      "x28    15.789757\n",
      "x29    14.974226\n",
      "x30    19.582782\n",
      "x31    27.791366\n",
      "x32     6.075373\n",
      "x33    12.160034\n",
      "x34     4.737683\n",
      "x35    12.644124\n",
      "x36    10.681589\n",
      "x37     7.426448\n",
      "x38    16.646885\n",
      "x39     0.001230\n",
      "dtype: float64\n",
      "8\n",
      "standard error= x1     40.104583\n",
      "x2     61.587498\n",
      "x3     55.999642\n",
      "x4     58.837873\n",
      "x5     60.544672\n",
      "x6     62.527688\n",
      "x7     59.158193\n",
      "x8     56.932100\n",
      "x9     10.495244\n",
      "x10     8.573248\n",
      "x11    27.491571\n",
      "x12    44.240900\n",
      "x13    22.299749\n",
      "x14    45.676270\n",
      "x15    31.169772\n",
      "x16    44.938797\n",
      "x17    40.200778\n",
      "x18    44.528827\n",
      "x19    41.803129\n",
      "x20    39.694707\n",
      "x21    50.695848\n",
      "x22    70.192341\n",
      "x23    39.010964\n",
      "x24    34.318637\n",
      "x25    44.545735\n",
      "x26    42.165672\n",
      "x27    41.359604\n",
      "x28    42.271501\n",
      "x29    40.088203\n",
      "x30    52.425984\n",
      "x31    74.401572\n",
      "x32    16.264666\n",
      "x33    32.554196\n",
      "x34    12.683475\n",
      "x35    33.850177\n",
      "x36    28.596183\n",
      "x37    19.881693\n",
      "x38    44.566157\n",
      "x39     0.003293\n",
      "dtype: float64\n",
      "9\n",
      "standard error= x1     50.450540\n",
      "x2     77.475499\n",
      "x3     70.446119\n",
      "x4     74.016541\n",
      "x5     76.163650\n",
      "x6     78.658233\n",
      "x7     74.419495\n",
      "x8     71.619127\n",
      "x9     13.202749\n",
      "x10    10.784927\n",
      "x11    34.583694\n",
      "x12    55.653922\n",
      "x13    28.052515\n",
      "x14    57.459581\n",
      "x15    39.210776\n",
      "x16    56.531859\n",
      "x17    50.571552\n",
      "x18    56.016127\n",
      "x19    52.587268\n",
      "x20    49.934928\n",
      "x21    63.774081\n",
      "x22    88.300172\n",
      "x23    49.074796\n",
      "x24    43.171969\n",
      "x25    56.037396\n",
      "x26    53.043338\n",
      "x27    52.029326\n",
      "x28    53.176468\n",
      "x29    50.429936\n",
      "x30    65.950548\n",
      "x31    93.595276\n",
      "x32    20.460534\n",
      "x33    40.952347\n",
      "x34    15.955487\n",
      "x35    42.582658\n",
      "x36    35.973267\n",
      "x37    25.010662\n",
      "x38    56.063087\n",
      "x39     0.004142\n",
      "dtype: float64\n",
      "10\n",
      "standard error= x1     114.619734\n",
      "x2     176.018355\n",
      "x3     160.048146\n",
      "x4     168.159869\n",
      "x5     173.037936\n",
      "x6     178.705436\n",
      "x7     169.075349\n",
      "x8     162.713129\n",
      "x9      29.995626\n",
      "x10     24.502522\n",
      "x11     78.571485\n",
      "x12    126.441416\n",
      "x13     63.733150\n",
      "x14    130.543733\n",
      "x15     89.083858\n",
      "x16    128.436021\n",
      "x17    114.894663\n",
      "x18    127.264317\n",
      "x19    119.474215\n",
      "x20    113.448302\n",
      "x21    144.889791\n",
      "x22    200.611176\n",
      "x23    111.494149\n",
      "x24     98.083382\n",
      "x25    127.312640\n",
      "x26    120.510370\n",
      "x27    118.206613\n",
      "x28    120.812832\n",
      "x29    114.572922\n",
      "x30    149.834557\n",
      "x31    212.641245\n",
      "x32     46.484755\n",
      "x33     93.040573\n",
      "x34     36.249635\n",
      "x35     96.744513\n",
      "x36     81.728487\n",
      "x37     56.822294\n",
      "x38    127.371007\n",
      "x39      0.009411\n",
      "dtype: float64\n",
      "11\n",
      "standard error= x1     34.427703\n",
      "x2     52.869672\n",
      "x3     48.072787\n",
      "x4     50.509261\n",
      "x5     51.974459\n",
      "x6     53.676776\n",
      "x7     50.784239\n",
      "x8     48.873254\n",
      "x9      9.009622\n",
      "x10     7.359689\n",
      "x11    23.600088\n",
      "x12    37.978517\n",
      "x13    19.143178\n",
      "x14    39.210708\n",
      "x15    26.757632\n",
      "x16    38.577626\n",
      "x17    34.510282\n",
      "x18    38.225688\n",
      "x19    35.885817\n",
      "x20    34.075847\n",
      "x21    43.519755\n",
      "x22    60.256483\n",
      "x23    33.488888\n",
      "x24    29.460770\n",
      "x25    38.240202\n",
      "x26    36.197042\n",
      "x27    35.505074\n",
      "x28    36.287890\n",
      "x29    34.413643\n",
      "x30    45.004987\n",
      "x31    63.869889\n",
      "x32    13.962372\n",
      "x33    27.946089\n",
      "x34    10.888105\n",
      "x35    29.058621\n",
      "x36    24.548339\n",
      "x37    17.067402\n",
      "x38    38.257734\n",
      "x39     0.002827\n",
      "dtype: float64\n",
      "12\n",
      "standard error= x1     20.768245\n",
      "x2     31.893220\n",
      "x3     28.999536\n",
      "x4     30.469320\n",
      "x5     31.353190\n",
      "x6     32.380099\n",
      "x7     30.635199\n",
      "x8     29.482412\n",
      "x9      5.434985\n",
      "x10     4.439675\n",
      "x11    14.236570\n",
      "x12    22.910246\n",
      "x13    11.547974\n",
      "x14    23.653556\n",
      "x15    16.141334\n",
      "x16    23.271654\n",
      "x17    20.818060\n",
      "x18    23.059350\n",
      "x19    21.647841\n",
      "x20    20.555990\n",
      "x21    26.252955\n",
      "x22    36.349256\n",
      "x23    20.201912\n",
      "x24    17.771981\n",
      "x25    23.068106\n",
      "x26    21.835585\n",
      "x27    21.418161\n",
      "x28    21.890389\n",
      "x29    20.759763\n",
      "x30    27.148910\n",
      "x31    38.529016\n",
      "x32     8.422693\n",
      "x33    16.858261\n",
      "x34     6.568165\n",
      "x35    17.529388\n",
      "x36    14.808595\n",
      "x37    10.295778\n",
      "x38    23.078681\n",
      "x39     0.001705\n",
      "dtype: float64\n",
      "13\n",
      "standard error= x1      87.741564\n",
      "x2     134.742293\n",
      "x3     122.517075\n",
      "x4     128.726611\n",
      "x5     132.460778\n",
      "x6     136.799257\n",
      "x7     129.427412\n",
      "x8     124.557124\n",
      "x9      22.961693\n",
      "x10     18.756714\n",
      "x11     60.146580\n",
      "x12     96.791078\n",
      "x13     48.787814\n",
      "x14     99.931407\n",
      "x15     68.193816\n",
      "x16     98.317951\n",
      "x17     87.952022\n",
      "x18     97.421010\n",
      "x19     91.457676\n",
      "x20     86.844831\n",
      "x21    110.913334\n",
      "x22    153.568131\n",
      "x23     85.348924\n",
      "x24     75.082964\n",
      "x25     97.458001\n",
      "x26     92.250854\n",
      "x27     90.487324\n",
      "x28     92.482389\n",
      "x29     87.705729\n",
      "x30    114.698559\n",
      "x31    162.777165\n",
      "x32     35.584144\n",
      "x33     71.222687\n",
      "x34     27.749146\n",
      "x35     74.058058\n",
      "x36     62.563269\n",
      "x37     43.497544\n",
      "x38     97.502681\n",
      "x39      0.007204\n",
      "dtype: float64\n",
      "14\n",
      "standard error= x1      58.405866\n",
      "x2      89.692272\n",
      "x3      81.554460\n",
      "x4      85.687886\n",
      "x5      88.173564\n",
      "x6      91.061507\n",
      "x7      86.154380\n",
      "x8      82.912434\n",
      "x9      15.284633\n",
      "x10     12.485555\n",
      "x11     40.037046\n",
      "x12     64.429746\n",
      "x13     32.475994\n",
      "x14     66.520131\n",
      "x15     45.393753\n",
      "x16     65.446121\n",
      "x17     58.545959\n",
      "x18     64.849065\n",
      "x19     60.879525\n",
      "x20     57.808948\n",
      "x21     73.830338\n",
      "x22    102.223840\n",
      "x23     56.813186\n",
      "x24     49.979568\n",
      "x25     64.873689\n",
      "x26     61.407510\n",
      "x27     60.233603\n",
      "x28     61.561633\n",
      "x29     58.382012\n",
      "x30     76.350002\n",
      "x31    108.353907\n",
      "x32     23.686867\n",
      "x33     47.409944\n",
      "x34     18.471438\n",
      "x35     49.297331\n",
      "x36     41.645735\n",
      "x37     28.954484\n",
      "x38     64.903430\n",
      "x39      0.004795\n",
      "dtype: float64\n",
      "15\n",
      "standard error= x1     518.019647\n",
      "x2     795.508442\n",
      "x3     723.331670\n",
      "x4     759.992305\n",
      "x5     782.038546\n",
      "x6     807.652605\n",
      "x7     764.129784\n",
      "x8     735.375966\n",
      "x9     135.564123\n",
      "x10    110.738240\n",
      "x11    355.100918\n",
      "x12    571.447299\n",
      "x13    288.039611\n",
      "x14    589.987569\n",
      "x15    402.611200\n",
      "x16    580.461845\n",
      "x17    519.262175\n",
      "x18    575.166371\n",
      "x19    539.959292\n",
      "x20    512.725403\n",
      "x21    654.824048\n",
      "x22    906.654784\n",
      "x23    503.893680\n",
      "x24    443.284216\n",
      "x25    575.384766\n",
      "x26    544.642158\n",
      "x27    534.230411\n",
      "x28    546.009122\n",
      "x29    517.808081\n",
      "x30    677.171733\n",
      "x31    961.024236\n",
      "x32    210.086130\n",
      "x33    420.493427\n",
      "x34    163.828885\n",
      "x35    437.233246\n",
      "x36    369.368872\n",
      "x37    256.806256\n",
      "x38    575.648551\n",
      "x39      0.042531\n",
      "dtype: float64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-231392254592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlist_incomechange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscenarios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_incomechange\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mfit_predict_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-d641c258253f>\u001b[0m in \u001b[0;36mfit_predict_cluster\u001b[0;34m(i, y, cluster_number, key)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# print(\"cross_val_score: %.2f (%.2f)\" % (results1.mean(), results1.std()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# print(\"oob_r2_score: %.2f \" % results0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"r2_score: %.2f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mresults2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean_squared_error: %.2f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mresults3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"explained_variance_score: %.2f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mresults4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results2' is not defined"
     ]
    }
   ],
   "source": [
    "# CLUSTER of MONTHS - PREDICTIONS\n",
    "for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "    print(cluster_number)\n",
    "    for j in range(0, iter_n):\n",
    "        for key in scenarios:\n",
    "            list_incomechange=[0,scenarios[key]]\n",
    "            for y in list_incomechange:\n",
    "                fit_predict_cluster(j,y,cluster_number,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.POSTPROCESSING <a id = \"post\"></a>\n",
    "    \n",
    "<a href=\"#toc\">back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Average of the clustered predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if option_deseason == 'month-ind': \n",
    "    df_habe_outliers = pd.read_csv('preprocessing/'+directory_name+'/'+option_deseason+'/4_habe_deseasonal_'+\n",
    "                                            str(cluster_number)+ '_short.csv', delimiter=',')\n",
    "if option_deseason == 'deseasonal':\n",
    "    df_habe_outliers = pd.read_csv('preprocessing/'+directory_name+'/2_habe_rename_removeoutliers.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pandas_cluster(y,cluster_number,key):\n",
    "    df_all = []\n",
    "    df_trained_partner = pd.read_csv('preprocessing/'+directory_name+'/checks/'+option_deseason+'/train_partner_independent_'+\n",
    "                                     model_name+'_'+str(y)+'.csv')\n",
    "    for i in range(0,iter_n):\n",
    "        df = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + \n",
    "                         str(y) + '_' + str(i) + '_' + \n",
    "                         str(cluster_number) + '.csv', delimiter = ',', header=None)\n",
    "        df_all.append(df)\n",
    "    glued = pd.concat(df_all, axis=1, keys=list(map(chr,range(97,97+iter_n))))\n",
    "    glued = glued.swaplevel(0, 1, axis=1)\n",
    "    glued = glued.groupby(level=0, axis=1).mean()\n",
    "    glued_new = glued.reindex(columns=df_all[0].columns)\n",
    "\n",
    "    max_income = df_habe_outliers[['disposable_income']].quantile([0.99]).values[0]\n",
    "    min_income = df_habe_outliers[['disposable_income']].quantile([0.01]).values[0]\n",
    "    glued_new['income'] = df_trained_partner[df_trained_partner.columns[-1]]\n",
    "    pd.DataFrame.to_csv(glued_new, 'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y)\n",
    "                        + '_'+str(cluster_number)+'.csv', sep=',',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in scenarios:\n",
    "    list_incomechange=[0,scenarios[key]]\n",
    "    for y in list_incomechange:\n",
    "        for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "            average_pandas_cluster(y,cluster_number,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_categories_cluster(y,cluster_number):\n",
    "    df_income = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y)\n",
    "                            + '_'+str(cluster_number)+'.csv', \n",
    "                            sep=',',header=None)\n",
    "#     df_income['household_size'] = df_income.iloc[:, [17]]\n",
    "    df_income['income'] = df_income.iloc[:, [16]]\n",
    "    df_income['food'] = df_income.iloc[:,[0,1,2]].sum(axis=1)\n",
    "    df_income['misc'] = df_income.iloc[:,[3,4]].sum(axis=1)\n",
    "    df_income['housing'] = df_income.iloc[:, [5, 6]].sum(axis=1)\n",
    "    df_income['services'] = df_income.iloc[:, [7, 8, 9 ]].sum(axis=1)\n",
    "    df_income['travel'] = df_income.iloc[:, [10, 11, 12, 13, 14]].sum(axis=1)\n",
    "    df_income['savings'] = df_income.iloc[:, [15]]             \n",
    "    df_income = df_income[['income','food','misc','housing','services','travel','savings']]\n",
    "    pd.DataFrame.to_csv(df_income,\n",
    "                        'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y) \n",
    "                        + '_'+str(cluster_number)+'_aggregated.csv', sep=',',index=False)\n",
    "    return df_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in scenarios:\n",
    "    list_incomechange=[0,scenarios[key]]\n",
    "    for y in list_incomechange: \n",
    "        for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "            accumulate_categories_cluster(y,cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation of clusters\n",
    "\n",
    "list_dfs_month=[]\n",
    "for key in scenarios:\n",
    "    list_incomechange=[0,scenarios[key]]\n",
    "    for y in list_incomechange:    \n",
    "        for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "            pd_predicted_month = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y) \n",
    "                                             + '_'+str(cluster_number)+'_aggregated.csv', delimiter = ',')\n",
    "            list_dfs_month.append(pd_predicted_month)\n",
    "\n",
    "        df_concat = pd.concat(list_dfs_month,sort=False)\n",
    "\n",
    "        by_row_index = df_concat.groupby(df_concat.index)\n",
    "        df_means = by_row_index.mean()\n",
    "        pd.DataFrame.to_csv(df_means,'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_' + str(y) + '_' + \n",
    "                            str(dependentsize) +'_aggregated.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Calculate differences/ rebounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dependent_columns = pd.read_csv(dependent_indices, delimiter=',', encoding='ISO-8859–1')['name'].to_list()\n",
    "\n",
    "def difference_new():\n",
    "    for cluster_number in list(range(1,cluster_number_length+1)):\n",
    "        for key in scenarios:\n",
    "            list_incomechange=[0,scenarios[key]]\n",
    "            for i in range(0,iter_n):\n",
    "                df_trained_partner = pd.read_csv('preprocessing/'+directory_name+'/checks/'+'/'+option_deseason+'/train_partner_independent_'+\n",
    "                                     model_name+'_'+str(y)+'.csv',header=None)\n",
    "                df_500 = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_'\n",
    "                                     +str(list_incomechange[1])+ '_'+str(i)\n",
    "                                     + '_'+str(cluster_number)+'.csv', delimiter=',',header=None)\n",
    "                df_0 = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name + '_0_' \n",
    "                                    + str(i)  + '_'+str(cluster_number)+ '.csv', delimiter=',',header=None)\n",
    "                df_500.columns = list_dependent_columns\n",
    "                df_0.columns = df_500.columns\n",
    "                df_diff = -df_500+df_0\n",
    "                if option_deseason == 'month-ind':\n",
    "                    df_diff['disposable_income']=df_trained_partner[df_trained_partner.columns[-25]]\n",
    "                if option_deseason == 'deseasonal':\n",
    "                    df_diff['disposable_income']=df_trained_partner[df_trained_partner.columns[-1]]\n",
    "                pd.DataFrame.to_csv(df_diff,'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_' + model_name \n",
    "                                    + '_rebound_'+str(i)+ '_' + str(cluster_number) + '.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_clusters(key):\n",
    "    df_all = []\n",
    "    for i in range(0,iter_n):\n",
    "        df = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+ model_name + '_rebound_' + \n",
    "                         str(i)+ '_' + str(cluster_number)+'.csv',delimiter=',',index_col=None)\n",
    "        df_all.append(df)\n",
    "    \n",
    "    df_concat = pd.concat(df_all,sort=False)\n",
    "\n",
    "    by_row_index = df_concat.groupby(df_concat.index)\n",
    "    df_means = by_row_index.mean()\n",
    "    pd.DataFrame.to_csv(df_means, 'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name +'_rebound.csv',\n",
    "                        sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in scenarios:\n",
    "    average_clusters(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_categories(key):\n",
    "    df_income = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound.csv',delimiter=',')\n",
    "#     df_income['household_size'] = df_income.iloc[:, [17]]\n",
    "    df_income['income'] = df_income.iloc[:, [16]]\n",
    "    df_income['food'] = df_income.iloc[:,[0,1,2]].sum(axis=1)\n",
    "    df_income['misc'] = df_income.iloc[:,[3,4]].sum(axis=1)\n",
    "    df_income['housing'] = df_income.iloc[:, [5, 6]].sum(axis=1)\n",
    "    df_income['services'] = df_income.iloc[:, [7, 8, 9]].sum(axis=1)\n",
    "    df_income['travel'] = df_income.iloc[:, [10, 11, 12,13, 14]].sum(axis=1)\n",
    "    df_income['savings'] = df_income.iloc[:, [15]]    \n",
    "    df_income = df_income[['income','food','misc','housing','services','travel','savings']]#'transfers','total_sum'\n",
    "    data[key]=list(df_income.mean())\n",
    "    if list(scenarios.keys()).index(key) == len(scenarios)-1:\n",
    "        df = pd.DataFrame(data, columns = [key for key in scenarios],\n",
    "                  index=['income','food','misc','housing','services','travel','savings'])\n",
    "        print(df)\n",
    "        pd.DataFrame.to_csv(df.T, 'postprocessing/rebound_results_'+directory_name+ '_income.csv', sep=',',index=True)\n",
    "    pd.DataFrame.to_csv(df_income,\n",
    "                        'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound_aggregated.csv',\n",
    "                        sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          baseline_2011\n",
      "income      4589.453636\n",
      "food          74.577799\n",
      "misc          42.708161\n",
      "housing       64.318477\n",
      "services      11.483421\n",
      "travel       109.763693\n",
      "savings      184.740454\n"
     ]
    }
   ],
   "source": [
    "data={}\n",
    "for key in scenarios:\n",
    "    accumulate_categories(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=('<2000','2000-4000','4000-6000','6000-8000','8000-10000','>10000')\n",
    "def income_group(row):\n",
    "    if row['disposable_income'] <= 2000:\n",
    "        return groups[0]\n",
    "    if row['disposable_income'] <= 4000:\n",
    "        return groups[1]\n",
    "    if row['disposable_income'] <= 6000:\n",
    "        return groups[2]\n",
    "    if row['disposable_income'] <= 8000:\n",
    "        return groups[3]\n",
    "    if row['disposable_income'] <= 10000:\n",
    "        return groups[4]\n",
    "    if row['disposable_income'] > 10000:\n",
    "        return groups[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_income_groups():\n",
    "    df_income = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound.csv',\n",
    "                            delimiter=',')\n",
    "    df_income['income_group'] = df_income.apply(lambda row: income_group(row), axis=1)\n",
    "    df_income_new = df_income.groupby(['income_group']).mean()\n",
    "    pd.DataFrame.to_csv(df_income_new,'postprocessing/rebound_results_'+directory_name+ '_income_categories.csv', sep=',',index=True)\n",
    "    pd.DataFrame.to_csv(df_income,\n",
    "                        'postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound_income.csv',\n",
    "                        sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_income_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=('<2000','2000-4000','4000-6000','6000-8000','8000-10000','>10000')\n",
    "def income_group(row):\n",
    "    if row['income'] <= 2000 :\n",
    "        return groups[0]\n",
    "    if row['income'] <= 4000:\n",
    "        return groups[1]\n",
    "    if row['income'] <= 6000:\n",
    "        return groups[2]\n",
    "    if row['income'] <= 8000:\n",
    "        return groups[3]\n",
    "    if row['income'] <= 10000:\n",
    "        return groups[4]\n",
    "    if row['income'] > 10000:\n",
    "        return groups[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_income_groups_new():\n",
    "    df_income = pd.read_csv('postprocessing/'+directory_name+'/'+option_deseason+'/'+key+'/predicted_'+model_name+ '_rebound_aggregated.csv',\n",
    "                            delimiter=',')\n",
    "    print(df_income.columns)\n",
    "    df_income['income_group'] = df_income.apply(lambda row: income_group(row), axis=1)\n",
    "    df_income_new = df_income.groupby(['income_group']).mean()\n",
    "    pd.DataFrame.to_csv(df_income_new,'postprocessing/rebound_results_'+directory_name+ '_categories.csv', sep=',',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['income', 'food', 'misc', 'housing', 'services', 'travel', 'savings'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "accumulate_income_groups_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LCA <a id = \"lca\"></a>\n",
    "\n",
    "<a href = '#toc'>back</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a file with associated impacts_per_FU for each HABE category: \n",
    "    - a. Get the ecoinvent data from brightway\n",
    "    - b. Get the exiobase data from direct file (Livia's)\n",
    "    - c. Attach the heia and Agribalyse values \n",
    "2. Convert the impact_per_FU to impact_per_expenses\n",
    "3. Run the following scripts to  \n",
    "    - (a) allocate the income category to each household in HBS (train data) and ABZ (target data) \n",
    "    - (b) calculate environmental impact per consumption main-category per income group as listed in the raw/dependent_10.csv\n",
    "        - (1) From HBS: % of expense of consumption sub-category per consumption main-category as listed in the raw/dependent_10.csv \n",
    "        - (2) expenses per FU of each consumption sub-category \n",
    "    - (c) From target data: Multiply the rebound results (consumption expenses) with the env. impact values above \n",
    "        based on the income of the household\n",
    "    \n",
    "    OR \n",
    "    \n",
    "    Use A.Kim's analysis here: https://github.com/aleksandra-kim/consumption_model for the calculation of impacts_per_FU for each HABE catergory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "file = open('LCA/contribution_scores_sectors_allfu1.pickle','rb')\n",
    "x = pickle.load(file)\n",
    "print(x)\n",
    "with open('LCA/impacts_per_FU_sectors.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x:\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "file = open('LCA/contribution_scores_5categories_allfu1.pickle','rb')\n",
    "x = pickle.load(file)\n",
    "print(x)\n",
    "with open('LCA/impacts_per_FU.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('LCA/contribution_scores_v2.pickle','rb')\n",
    "x1 = pickle.load(file)\n",
    "with open('LCA/impacts_per_FU.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x1.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('LCA/contribution_scores_sectors_allfu1.pickle','rb')\n",
    "x = pickle.load(file)\n",
    "with open('LCA/impacts_per_FU_sectors.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## TODO use the manually updated CHF/FU to calculate the income per expense \n",
    "df_expense = pd.read_csv('LCA/impacts_per_expense.csv',sep=',',index_col='sector')\n",
    "df_income_CHF = pd.read_csv('postprocessing/rebound_results_'+directory_name+ '_income.csv',sep=',')\n",
    "for i in ['food','travel','housing','food','misc','services']:\n",
    "    df_income_CHF[i+'_GHG']=df_expense.loc[i,'Average of GWP/CHF']*df_income_CHF[i]\n",
    "    pd.DataFrame.to_csv(df_income_CHF,'postprocessing/rebound_results_'+directory_name+ '_income_all_GHG.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### USETOX Analysis for MAJA #########################\n",
    "file = open('LCA/USEtox/contribution_scores_sectors_usetox.pickle','rb')\n",
    "x1 = pickle.load(file)\n",
    "with open('LCA/impacts_per_FU_usetox.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in x1.items():\n",
    "        writer.writerow([key, value])\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rebound]",
   "language": "python",
   "name": "conda-env-rebound-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
